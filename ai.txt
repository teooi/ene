

##### FILE: ./llm/worker.py #####

import traceback
from PySide6.QtCore import QThread, Signal, QMutex, QMutexLocker

from logger import log
from .stream import llm_stream

class LLMWorker(QThread):
    """Thread-safe LLM processing"""

    text_chunk = Signal(str)
    finished = Signal()
    error = Signal(str)

    def __init__(self, llm, system_prompt, user_text, parent=None):
        super().__init__(parent)

        self.llm = llm
        self.system_prompt = system_prompt
        self.user_text = user_text

        self._running = True
        self._mutex = QMutex()

        log(f"LLMWorker created: '{user_text}'")

    def stop(self):
        with QMutexLocker(self._mutex):
            log("LLMWorker stop requested")
            self._running = False

    def is_running_safe(self) -> bool:
        with QMutexLocker(self._mutex):
            return self._running

    def run(self):
        log("LLMWorker started")
        buffer = ""
        chunk_count = 0

        try:
            for token in llm_stream(
                self.llm,
                self.system_prompt,
                self.user_text,
            ):
                if not self.is_running_safe():
                    log("LLMWorker stopped by flag")
                    break

                buffer += token

                if any(p in buffer for p in ".!?"):
                    chunk_count += 1
                    text = buffer.strip()
                    log(f"Emitting chunk #{chunk_count}: '{text}'")
                    self.text_chunk.emit(text)
                    buffer = ""

            if buffer.strip() and self.is_running_safe():
                chunk_count += 1
                self.text_chunk.emit(buffer.strip())

            log(f"LLMWorker completed ({chunk_count} chunks)")

        except Exception as e:
            msg = f"LLMWorker error: {e}"
            log(msg, "ERROR")
            traceback.print_exc()
            self.error.emit(msg)
        finally:
            self.finished.emit()


##### FILE: ./llm/__init__.py #####

from .model import init_llm, load_system_prompt
from .stream import llm_stream
from .worker import LLMWorker

__all__ = [
    "init_llm",
    "load_system_prompt",
    "llm_stream",
    "LLMWorker",
]



##### FILE: ./llm/stream.py #####

import time
import traceback
from logger import log

def llm_stream(llm, system_prompt, user_msg):
    if not llm:
        yield "LLM not loaded."
        return

    prompt = f"""<|im_start|>system
{system_prompt}
<|im_end|>
<|im_start|>user
{user_msg}
<|im_end|>
<|im_start|>assistant
"""

    start = time.perf_counter()
    first_token = None
    token_count = 0

    try:
        for chunk in llm(
            prompt,
            max_tokens=256,
            temperature=0.7,
            top_p=0.9,
            stop=["<|im_end|>"],
            stream=True,
        ):
            text = chunk["choices"][0]["text"]
            if not text:
                continue

            if first_token is None:
                first_token = time.perf_counter()
                log(f"First token: {first_token - start:.3f}s")

            token_count += 1
            yield text

    except Exception as e:
        log(f"LLM stream error: {e}", "ERROR")
        traceback.print_exc()
        yield " ...error."


##### FILE: ./llm/model.py #####

from pathlib import Path
import traceback
from llama_cpp import Llama
from typing import Optional
from config import MODEL_PATH
from logger import log

def init_llm() -> Optional[Llama]:
    try:
        log(f"Initializing Llama model: {MODEL_PATH}")
        llm = Llama(
            model_path=MODEL_PATH,
            n_ctx=8192,
            n_gpu_layers=-1,
            n_threads=8,
            verbose=False,
        )
        log("Llama model loaded successfully")
        return llm
    except Exception as e:
        log(f"Failed to initialize LLM: {e}", "ERROR")
        traceback.print_exc()
        return None

def load_system_prompt() -> str:
    try:
        return Path("system_prompt.txt").read_text(encoding="utf-8").strip()
    except FileNotFoundError:
        log("system_prompt.txt not found", "WARNING")
        return "You are a helpful AI assistant."


##### FILE: ./config.py #####

# config.py
WINDOW_WIDTH = 500
WINDOW_HEIGHT = 500
FPS = 60

LIP_SYNC_SENSITIVITY = 0.7
LIP_SYNC_SMOOTHING = 0.35
LIP_SYNC_THRESHOLD = 0.005

TTS_VOICE = "af_heart"
SAMPLE_RATE = 22050
FRAME_SIZE = 441

AUDIO_NOISE_FLOOR = 1e-4
AUDIO_VOLUME_GAIN = 6.0
AUDIO_COMPRESSION = 0.8

MODEL_PATH = "models/DeepSeek-R1-Distill-Llama-8B-Q4_1.gguf"
LIVE2D_MODEL_PATH = "live2d_models/pichu/Pichu.model3.json"

DEBUG = True


##### FILE: ./input/input_thread.py #####

# input/input_thread.py

from __future__ import annotations

import sys
import traceback

from PySide6.QtCore import (
    QThread,
    Signal,
    QMutex,
    QMutexLocker,
    Qt,
    QCoreApplication,
)

from logger import log

class InputThread(QThread):
    """Thread-safe console input"""
    text_received = Signal(str)
    error = Signal(str)

    def __init__(self):
        super().__init__()
        self.running = True
        self._mutex = QMutex()
        log("InputThread initialized")

    def stop(self):
        """Thread-safe stop"""
        with QMutexLocker(self._mutex):
            log("InputThread stop requested")
            self.running = False

    def is_running_safe(self) -> bool:
        """Thread-safe running check"""
        with QMutexLocker(self._mutex):
            return self.running

    def run(self):
        print("\n" + "="*60)
        print("KOKORO AI COMPANION")
        print("="*60)
        print("Type text and press Enter to talk")
        print("Commands: /quit or /exit to quit")
        print("="*60 + "\n")
        
        log("InputThread started, waiting for input")
        
        while self.is_running_safe():
            try:
                sys.stdout.flush()
                text = input("> ")
                
                if not text:
                    continue

                log(f"Input received: '{text}'")

                # Handle commands
                if text.strip().lower() in ("/quit", "/exit"):
                    log("Quit command received")
                    QCoreApplication.quit()
                    break

                self.text_received.emit(text)
                
            except EOFError:
                log("EOF received", "INFO")
                break
            except KeyboardInterrupt:
                log("KeyboardInterrupt received", "INFO")
                break
            except Exception as e:
                error_msg = f"InputThread error: {e}"
                log(error_msg, "ERROR")
                traceback.print_exc()
                self.error.emit(error_msg)
                break

        log("InputThread exited")


##### FILE: ./input/__init__.py #####

from .input_thread import InputThread

__all__ = [
    "InputThread",
]



##### FILE: ./tests/live2d_app.py #####

import sys
import os
import math
import time
import json
from pathlib import Path
import queue
import signal
import traceback
from typing import Optional

import numpy as np
import sounddevice as sd

from PySide6.QtWidgets import QApplication
from PySide6.QtOpenGLWidgets import QOpenGLWidget
from PySide6.QtCore import Qt, QTimer, QThread, Signal, QCoreApplication, QMutex, QMutexLocker
from PySide6.QtGui import QSurfaceFormat, QGuiApplication

from OpenGL.GL import *
import live2d.v3 as live2d

from kokoro import KPipeline
from llama_cpp import Llama

import librosa

# ============================================================
# CONFIGURATION
# ============================================================
WINDOW_WIDTH = 500
WINDOW_HEIGHT = 500
FPS = 60

# Lip sync parameters
LIP_SYNC_SENSITIVITY = 0.7
LIP_SYNC_SMOOTHING = 0.35
LIP_SYNC_THRESHOLD = 0.005

# TTS settings
TTS_VOICE = "af_heart"
SAMPLE_RATE = 22050
FRAME_SIZE = 441

# Audio processing
AUDIO_NOISE_FLOOR = 1e-4
AUDIO_VOLUME_GAIN = 6.0
AUDIO_COMPRESSION = 0.8

# Model paths
MODEL_PATH = "models/Meta-Llama-3-8B-Instruct.Q4_1.gguf"
LIVE2D_MODEL_PATH = "live2d_models/l2d/L2DZeroVS.model3.json"

# Debug mode
DEBUG = True

def log(msg: str, level: str = "DEBUG"):
    """Thread-safe logging"""
    if DEBUG or level != "DEBUG":
        print(f"[{level}] {msg}", flush=True)

# ============================================================
# LLM INITIALIZATION
# ============================================================
def init_llm() -> Optional[Llama]:
    """Initialize LLM with error handling"""
    try:
        log(f"Initializing Llama model: {MODEL_PATH}")
        llm = Llama(
            model_path=MODEL_PATH,
            n_ctx=8192,
            n_gpu_layers=-1,
            n_threads=8,
            verbose=False,
        )
        log("Llama model loaded successfully")
        return llm
    except Exception as e:
        log(f"Failed to initialize LLM: {e}", "ERROR")
        traceback.print_exc()
        return None

llm = init_llm()

def load_system_prompt() -> str:
    """Load system prompt with fallback"""
    try:
        prompt = Path("system_prompt.txt").read_text(encoding="utf-8").strip()
        log(f"System prompt loaded ({len(prompt)} chars)")
        return prompt
    except FileNotFoundError:
        log("system_prompt.txt not found, using default", "WARNING")
        return "You are a helpful AI assistant named Ene."
    except Exception as e:
        log(f"Error loading system prompt: {e}", "ERROR")
        return "You are a helpful AI assistant."

SYSTEM_PROMPT = load_system_prompt()

# ============================================================
# LLM STREAMING
# ============================================================
def llm_stream(user_msg: str):
    """Stream LLM response with comprehensive error handling"""
    if not llm:
        log("LLM not initialized, cannot stream", "ERROR")
        yield "I'm sorry, my language model isn't loaded."
        return
    
    log(f"llm_stream called: '{user_msg[:50]}...'")
    
    prompt = f"""<|im_start|>system
{SYSTEM_PROMPT}
<|im_end|>
<|im_start|>user
{user_msg}
<|im_end|>
<|im_start|>assistant
"""

    start = time.perf_counter()
    first_token = None
    token_count = 0
    spoken_chunks = []

    try:
        for chunk in llm(
            prompt,
            max_tokens=256,
            temperature=0.7,
            top_p=0.9,
            stop=["<|im_end|>"],
            stream=True,
        ):
            text = chunk["choices"][0]["text"]
            if not text:
                continue

            if first_token is None:
                first_token = time.perf_counter()
                log(f"First token: {first_token - start:.3f}s")

            token_count += 1
            spoken_chunks.append(text)
            
            if DEBUG:
                print(f"[Token {token_count}] '{text}'", end='', flush=True)

            yield text

    except Exception as e:
        log(f"LLM stream error: {e}", "ERROR")
        traceback.print_exc()
        yield " ...I encountered an error."

    end = time.perf_counter()

    if first_token and token_count > 0:
        ttft = first_token - start
        tps = token_count / (end - first_token) if end > first_token else 0
        log(f"TTFT: {ttft:.3f}s | Tokens: {token_count} | {tps:.1f} tok/s | Total: {end - start:.2f}s", "INFO")

    llm_stream.last_spoken_text = "".join(spoken_chunks)

# ============================================================
# LLM WORKER THREAD
# ============================================================
class LLMWorker(QThread):
    """Thread-safe LLM processing"""
    text_chunk = Signal(str)
    finished = Signal()
    error = Signal(str)

    def __init__(self, user_text: str):
        super().__init__()
        self.user_text = user_text
        self._running = True
        self._mutex = QMutex()
        log(f"LLMWorker created: '{user_text}'")

    def stop(self):
        """Thread-safe stop"""
        with QMutexLocker(self._mutex):
            log("LLMWorker stop requested")
            self._running = False

    def is_running_safe(self) -> bool:
        """Thread-safe running check"""
        with QMutexLocker(self._mutex):
            return self._running

    def run(self):
        log("LLMWorker started")
        buffer = ""
        chunk_count = 0
        
        try:
            for token in llm_stream(self.user_text):
                if not self.is_running_safe():
                    log("LLMWorker stopped by flag")
                    break

                buffer += token
                
                # Emit on sentence boundaries
                if any(p in buffer for p in ".!?"):
                    chunk_count += 1
                    text = buffer.strip()
                    log(f"Emitting chunk #{chunk_count}: '{text}'")
                    self.text_chunk.emit(text)
                    buffer = ""

            # Emit remaining buffer
            if buffer.strip() and self.is_running_safe():
                chunk_count += 1
                log(f"Emitting final chunk #{chunk_count}: '{buffer.strip()}'")
                self.text_chunk.emit(buffer.strip())

            log(f"LLMWorker completed ({chunk_count} chunks)")
            
        except Exception as e:
            error_msg = f"LLMWorker error: {e}"
            log(error_msg, "ERROR")
            traceback.print_exc()
            self.error.emit(error_msg)
        finally:
            self.finished.emit()

# ============================================================
# HELPER FUNCTIONS
# ============================================================
def get_fixed_model_path(original_path: Path) -> Path:
    """Create fixed Live2D model file"""
    try:
        log(f"Processing model: {original_path}")
        fixed_path = original_path.with_stem(original_path.stem + "_fixed")
        
        if fixed_path.exists():
            log(f"Using cached fixed model: {fixed_path}")
            return fixed_path

        log("Creating fixed model file...")
        with open(original_path, "r", encoding="utf-8") as f:
            data = json.load(f)

        data.pop("DefaultExpression", None)
        if "FileReferences" in data:
            data["FileReferences"].pop("DefaultExpression", None)

        with open(fixed_path, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=4)

        log(f"Fixed model saved: {fixed_path}")
        return fixed_path
        
    except Exception as e:
        log(f"Error fixing model path: {e}", "ERROR")
        traceback.print_exc()
        return original_path
    
# ============================================================
# AUDIO HELPER FUNCTIONS
# ============================================================

def pitch_resample(audio: np.ndarray, semitones: float = 1.0) -> np.ndarray:
    """
    Raise the pitch of audio by resampling.
    Positive semitones = higher pitch.

    Parameters:
        audio (np.ndarray): float32 audio array
        semitones (float): semitone shift
    Returns:
        np.ndarray: pitch-shifted audio
    """
    # Compute resampling factor
    factor = 2 ** (semitones / 12)

    # New length after resampling
    new_len = int(len(audio) / factor)

    # Interpolate to new length
    resampled = np.interp(
        np.linspace(0, len(audio), new_len),
        np.arange(len(audio)),
        audio
    ).astype(np.float32)

    return resampled

# ============================================================
# AUDIO WORKER
# ============================================================
class AudioWorker(QThread):
    """Thread-safe audio playback with RMS tracking"""
    volume_update = Signal(float)
    error = Signal(str)

    def __init__(self):
        super().__init__()
        self.running = True
        self.audio_queue = queue.Queue()
        self._mutex = QMutex()
        log("AudioWorker initialized")

    def add_audio(self, audio: np.ndarray):
        """Thread-safe audio queueing"""
        try:
            if audio.dtype != np.float32:
                audio = audio.astype(np.float32)
            
            self.audio_queue.put(audio)
            log(f"Audio queued: {len(audio)} samples (queue size: {self.audio_queue.qsize()})")
        except Exception as e:
            log(f"Error queueing audio: {e}", "ERROR")

    def stop(self):
        """Thread-safe stop"""
        with QMutexLocker(self._mutex):
            log("AudioWorker stop requested")
            self.running = False

    def is_running_safe(self) -> bool:
        """Thread-safe running check"""
        with QMutexLocker(self._mutex):
            return self.running

    def run(self):
        log("AudioWorker started")
        frame_count = 0
        
        try:
            with sd.OutputStream(
                samplerate=SAMPLE_RATE,
                channels=1,
                dtype="float32",
                blocksize=FRAME_SIZE
            ) as stream:
                log(f"Audio stream opened (SR={SAMPLE_RATE}, BS={FRAME_SIZE})")

                while self.is_running_safe():
                    try:
                        audio = self.audio_queue.get(timeout=0.05)
                        log(f"Processing audio: {len(audio)} samples")

                        # Process in blocks
                        for i in range(0, len(audio), FRAME_SIZE):
                            if not self.is_running_safe():
                                log("AudioWorker stopped during playback")
                                break

                            frame = audio[i:i + FRAME_SIZE]
                            if len(frame) == 0:
                                continue

                            # Remove DC offset
                            frame = frame - np.mean(frame)
                            
                            # Play audio
                            stream.write(frame.reshape(-1, 1))

                            # Calculate RMS
                            rms = float(np.sqrt(np.mean(frame ** 2)))
                            
                            frame_count += 1
                            if frame_count % 100 == 0:
                                log(f"Frame {frame_count}, RMS: {rms:.6f}")

                            self.volume_update.emit(rms)

                    except queue.Empty:
                        # No audio, emit silence
                        self.volume_update.emit(0.0)

        except Exception as e:
            error_msg = f"AudioWorker error: {e}"
            log(error_msg, "ERROR")
            traceback.print_exc()
            self.error.emit(error_msg)

        log("AudioWorker exited")

    def stop_and_wait(self):
        """Safely stop and wait for thread"""
        self.stop()
        if not self.wait(5000):  # 5 second timeout
            log("AudioWorker did not stop in time", "WARNING")

# ============================================================
# TTS GENERATOR
# ============================================================
class TTSGenerator(QThread):
    """Thread-safe TTS generation"""
    audio_chunk = Signal(np.ndarray)
    finished_tts = Signal()
    error = Signal(str)

    def __init__(self, pipeline: KPipeline, text: str):
        super().__init__()
        self.pipeline = pipeline
        self.text = text
        self._mutex = QMutex()
        self._running = True
        log(f"TTSGenerator created: '{text}'")

    def stop(self):
        """Thread-safe stop"""
        with QMutexLocker(self._mutex):
            self._running = False

    def is_running_safe(self) -> bool:
        """Thread-safe running check"""
        with QMutexLocker(self._mutex):
            return self._running

    def run(self):
        log(f"TTSGenerator started: '{self.text}'")
        chunk_count = 0
        
        try:
            for r in self.pipeline(self.text, voice=TTS_VOICE):
                if not self.is_running_safe():
                    log("TTSGenerator stopped early")
                    break
                
                chunk_count += 1
                audio = (
                    r.output.audio
                    .detach()
                    .cpu()
                    .numpy()
                    .astype(np.float32)
                )

                audio = pitch_resample(audio, semitones=5.0)  

                # Normalize
                audio = audio - np.mean(audio)
                np.clip(audio, -1.0, 1.0, out=audio)

                log(f"TTS chunk #{chunk_count}: {len(audio)} samples, range=[{audio.min():.3f}, {audio.max():.3f}]")
                self.audio_chunk.emit(audio)

            log(f"TTSGenerator completed ({chunk_count} chunks)")
            
        except Exception as e:
            error_msg = f"TTS error: {e}"
            log(error_msg, "ERROR")
            traceback.print_exc()
            self.error.emit(error_msg)
        finally:
            self.finished_tts.emit()

# ============================================================
# INPUT THREAD
# ============================================================
class InputThread(QThread):
    """Thread-safe console input"""
    text_received = Signal(str)
    error = Signal(str)

    def __init__(self):
        super().__init__()
        self.running = True
        self._mutex = QMutex()
        log("InputThread initialized")

    def stop(self):
        """Thread-safe stop"""
        with QMutexLocker(self._mutex):
            log("InputThread stop requested")
            self.running = False

    def is_running_safe(self) -> bool:
        """Thread-safe running check"""
        with QMutexLocker(self._mutex):
            return self.running

    def run(self):
        print("\n" + "="*60)
        print("KOKORO AI COMPANION")
        print("="*60)
        print("Type text and press Enter to talk")
        print("Commands: /quit or /exit to quit")
        print("="*60 + "\n")
        
        log("InputThread started, waiting for input")
        
        while self.is_running_safe():
            try:
                sys.stdout.flush()
                text = input("> ")
                
                if not text:
                    continue

                log(f"Input received: '{text}'")

                # Handle commands
                if text.strip().lower() in ("/quit", "/exit"):
                    log("Quit command received")
                    QCoreApplication.quit()
                    break

                self.text_received.emit(text)
                
            except EOFError:
                log("EOF received", "INFO")
                break
            except KeyboardInterrupt:
                log("KeyboardInterrupt received", "INFO")
                break
            except Exception as e:
                error_msg = f"InputThread error: {e}"
                log(error_msg, "ERROR")
                traceback.print_exc()
                self.error.emit(error_msg)
                break

        log("InputThread exited")

# ============================================================
# LIVE2D WIDGET
# ============================================================
class Live2DWidget(QOpenGLWidget):
    """Main widget with Live2D rendering and audio/TTS coordination"""
    
    def __init__(self):
        super().__init__()
        log("Live2DWidget initializing")

        # State
        self.model = None
        self.start_time = time.time()
        self.pipeline = None
        
        # Audio/mouth state
        self.mouth_raw = 0.0
        self.mouth_value = 0.0
        self._peak = 1e-6
        
        # TTS queue
        self.tts_queue = queue.Queue()
        self.tts_busy = False
        self._tts_start_time = None
        self._tts_mutex = QMutex()
        
        # First frame ghost fix
        self._first_frame_rendered = False
        self._ghost_fix_timer = None
        
        # Workers
        self.audio_worker = AudioWorker()
        self.audio_worker.volume_update.connect(self.on_volume)
        self.audio_worker.error.connect(self.on_error)
        
        # OpenGL format
        fmt = QSurfaceFormat()
        fmt.setRenderableType(QSurfaceFormat.OpenGL)
        fmt.setVersion(2, 1)
        fmt.setProfile(QSurfaceFormat.NoProfile)
        fmt.setAlphaBufferSize(8)
        self.setFormat(fmt)
        
        # Window setup
        flags = (
            Qt.FramelessWindowHint | 
            Qt.WindowStaysOnTopHint | 
            Qt.Window |
            Qt.BypassWindowManagerHint
        )
        self.setWindowFlags(flags)
        self.setFixedSize(WINDOW_WIDTH, WINDOW_HEIGHT)
        
        # Transparency attributes
        self.setAttribute(Qt.WA_TranslucentBackground)
        self.setAttribute(Qt.WA_NoSystemBackground)
        self.setAttribute(Qt.WA_OpaquePaintEvent, False)
        self.setAutoFillBackground(False)
        self.setUpdateBehavior(QOpenGLWidget.NoPartialUpdate)
        
        self.setMouseTracking(True)
        self.drag_position = None

        # Update timer
        self.timer = QTimer(self)
        self.timer.timeout.connect(self.update)
        self.timer.start(1000 // FPS)
        
        # macOS ghosting fix
        if sys.platform == "darwin":
            QGuiApplication.instance().applicationStateChanged.connect(self._on_app_state_changed)
        
        log(f"Live2DWidget initialized ({FPS} FPS)")

    def on_error(self, error_msg: str):
        """Handle errors from worker threads"""
        log(f"Widget received error: {error_msg}", "ERROR")

    def set_pipeline(self, pipeline: KPipeline):
        """Set TTS pipeline"""
        log("TTS pipeline set")
        self.pipeline = pipeline

    def enqueue_tts(self, text: str):
        """Handle new user input"""
        log(f"\n{'='*60}")
        log(f"New input: '{text}'")
        log('='*60)
        
        try:
            # Stop existing LLM worker
            if hasattr(self, "llm_worker") and self.llm_worker.isRunning():
                log("Stopping previous LLM worker")
                self.llm_worker.stop()
                if not self.llm_worker.wait(3000):
                    log("LLM worker did not stop in time", "WARNING")

            # Stop existing TTS
            if hasattr(self, "current_tts") and self.current_tts.isRunning():
                log("Stopping current TTS")
                self.current_tts.stop()
                if not self.current_tts.wait(3000):
                    log("TTS did not stop in time", "WARNING")
            
            # Clear TTS queue
            cleared = 0
            while not self.tts_queue.empty():
                try:
                    self.tts_queue.get_nowait()
                    cleared += 1
                except queue.Empty:
                    break
            
            if cleared > 0:
                log(f"Cleared {cleared} items from TTS queue")
            
            with QMutexLocker(self._tts_mutex):
                self.tts_busy = False

            # Start new LLM worker
            self.llm_worker = LLMWorker(text)
            self.llm_worker.text_chunk.connect(self.on_llm_chunk)
            self.llm_worker.finished.connect(self.on_llm_finished)
            self.llm_worker.error.connect(self.on_error)
            self.llm_worker.start()
            
            log("LLM worker started")
            
        except Exception as e:
            log(f"Error in enqueue_tts: {e}", "ERROR")
            traceback.print_exc()

    def on_llm_chunk(self, text: str):
        """Handle LLM chunk - add to TTS queue"""
        try:
            self.tts_queue.put(text)
            log(f"Added to TTS queue (size: {self.tts_queue.qsize()})")
            self.process_queue()
        except Exception as e:
            log(f"Error handling LLM chunk: {e}", "ERROR")

    def on_llm_finished(self):
        """Handle LLM completion"""
        log("LLM generation finished")
        self.process_queue()

    def process_queue(self):
        """Process next item in TTS queue"""
        with QMutexLocker(self._tts_mutex):
            is_busy = self.tts_busy
            queue_size = self.tts_queue.qsize()
        
        log(f"process_queue: busy={is_busy}, queue_size={queue_size}")
        
        if is_busy or queue_size == 0:
            return  
        
        try:
            # Stop any running TTS first
            if hasattr(self, "current_tts") and self.current_tts.isRunning():
                log("Stopping previous TTS")
                self.current_tts.stop()
                if not self.current_tts.wait(2000):
                    log("Previous TTS did not stop in time", "WARNING")

            with QMutexLocker(self._tts_mutex):
                self.tts_busy = True
            
            text = self.tts_queue.get()
            log(f"Starting TTS: '{text}'")

            if not self.pipeline:
                log("No TTS pipeline available", "ERROR")
                with QMutexLocker(self._tts_mutex):
                    self.tts_busy = False
                return

            self.current_tts = TTSGenerator(self.pipeline, text)
            self.current_tts.audio_chunk.connect(self.audio_worker.add_audio)
            self.current_tts.finished_tts.connect(self.on_tts_done)
            self.current_tts.error.connect(self.on_error)

            self._tts_start_time = time.time()
            self.current_tts.start()
            
        except Exception as e:
            log(f"Error in process_queue: {e}", "ERROR")
            traceback.print_exc()
            with QMutexLocker(self._tts_mutex):
                self.tts_busy = False

    def on_tts_done(self):
        """Handle TTS completion"""
        try:
            if self._tts_start_time:
                elapsed = time.time() - self._tts_start_time
                log(f"TTS completed in {elapsed:.2f}s", "INFO")

            self._tts_start_time = None
            
            with QMutexLocker(self._tts_mutex):
                self.tts_busy = False
            
            self.process_queue()
            
        except Exception as e:
            log(f"Error in on_tts_done: {e}", "ERROR")
            traceback.print_exc()

    def initializeGL(self):
        """Initialize OpenGL and Live2D"""
        try:
            log("Initializing OpenGL")
            live2d.glInit()
            glClearColor(0, 0, 0, 0)
            glEnable(GL_BLEND)
            glBlendFunc(GL_SRC_ALPHA, GL_ONE_MINUS_SRC_ALPHA)

            path = get_fixed_model_path(Path(LIVE2D_MODEL_PATH))
            log(f"Loading Live2D model: {path}")

            self.model = live2d.LAppModel()
            self.model.LoadModelJson(str(path))
            self.model.Resize(self.width(), self.height())
            
            log(f"Live2D model loaded ({self.width()}x{self.height()})")

            self.audio_worker.start()
            log("Audio worker started")
            
        except Exception as e:
            log(f"Error in initializeGL: {e}", "ERROR")
            traceback.print_exc()

    def resizeGL(self, w: int, h: int):
        """Handle window resizing"""
        if self.model:
            self.model.Resize(w, h)

    def on_volume(self, v_raw: float):
        """Process audio volume for lip sync"""
        try:
            # Floor noise
            v = max(0.0, v_raw - AUDIO_NOISE_FLOOR)

            # Compress and gain
            v = v * AUDIO_VOLUME_GAIN
            v = v ** AUDIO_COMPRESSION

            # Track peak
            self._peak = max(self._peak * 0.995, max(v, 1e-6))

            # Normalize
            v = v / (self._peak + 1e-6)
            v = min(max(v * LIP_SYNC_SENSITIVITY, 0.0), 1.0)

            # Smooth
            self.mouth_raw = max(self.mouth_raw * 0.85, v)
            
        except Exception as e:
            log(f"Error in on_volume: {e}", "ERROR")

    def _apply_first_frame_fix(self):
        """Restore model to normal size after initial ghost frame"""
        try:
            if self.model:
                # Reset to normal scale
                self.model.SetParameterValue("ParamBodyScaleX", 1.0)
                self.model.SetParameterValue("ParamBodyScaleY", 1.0)
                self._first_frame_rendered = True
                log("First frame ghost fix applied - model restored to normal size", "INFO")
        except Exception as e:
            log(f"Error in first frame fix: {e}", "ERROR")

    def paintGL(self):
        """Render Live2D model"""
        try:
            live2d.clearBuffer()
            if not self.model:
                return

            # First frame ghost fix for macOS Metal
            if not self._first_frame_rendered and sys.platform == "darwin":
                log("Rendering first frame at tiny scale to prevent ghost", "INFO")
                # Scale model to nearly invisible for first frame
                self.model.SetParameterValue("ParamBodyScaleX", 0.001)
                self.model.SetParameterValue("ParamBodyScaleY", 0.001)
                
                # Schedule restoration to normal size
                if not self._ghost_fix_timer:
                    self._ghost_fix_timer = QTimer(self)
                    self._ghost_fix_timer.setSingleShot(True)
                    self._ghost_fix_timer.timeout.connect(self._apply_first_frame_fix)
                    self._ghost_fix_timer.start(100)  # 100ms delay

            # Breathing animation
            t = time.time() - self.start_time
            self.model.SetParameterValue("PARAM_BREATH", (math.sin(t * 2) + 1) / 2)

            # Mouth animation
            target = min(self.mouth_raw, 1.0) if self.mouth_raw > LIP_SYNC_THRESHOLD else 0.0
            self.mouth_value += (target - self.mouth_value) * LIP_SYNC_SMOOTHING

            for p in ("PARAM_MOUTH_OPEN_Y", "ParamMouthOpenY"):
                self.model.SetParameterValue(p, self.mouth_value)

            self.model.Update()
            self.model.Draw()
            
        except Exception as e:
            log(f"Error in paintGL: {e}", "ERROR")

    def _on_app_state_changed(self, state):
        """Forces a redraw on app activation to fix ghosting on macOS"""
        if state != Qt.ApplicationActive or not self.isValid():
            return

        try:
            self.makeCurrent()
            glFinish()

            w, h = self.width(), self.height()
            super().resizeGL(w + 1, h + 1)
            super().resizeGL(w, h)

            for _ in range(2):
                glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT)
                live2d.clearBuffer()
                self.update()
                
            log("macOS ghost fix applied (app state changed)", "INFO")
        except Exception as e:
            log(f"Error in _on_app_state_changed: {e}", "ERROR")

    def showEvent(self, event):
        """Ensure first frame draws correctly"""
        super().showEvent(event)
        QTimer.singleShot(0, self.update)

    def closeEvent(self, event):
        """Clean shutdown"""
        log("Shutting down Live2DWidget")
        
        try:
            # Stop ghost fix timer
            if self._ghost_fix_timer:
                self._ghost_fix_timer.stop()
            
            # Stop LLM worker
            if hasattr(self, "llm_worker") and self.llm_worker.isRunning():
                log("Stopping LLM worker")
                self.llm_worker.stop()
                self.llm_worker.wait(3000)

            # Stop TTS
            if hasattr(self, "current_tts") and self.current_tts.isRunning():
                log("Stopping TTS")
                self.current_tts.stop()
                self.current_tts.wait(3000)

            # Stop audio worker
            log("Stopping audio worker")
            self.audio_worker.stop_and_wait()
            
            event.accept()
            log("Shutdown complete")
            
        except Exception as e:
            log(f"Error during shutdown: {e}", "ERROR")
            traceback.print_exc()
            event.accept()

    # Mouse event handlers for dragging
    def mousePressEvent(self, event):
        if event.button() == Qt.LeftButton:
            self.drag_position = (
                event.globalPosition().toPoint() - self.frameGeometry().topLeft()
            )
            event.accept()

    def mouseMoveEvent(self, event):
        if event.buttons() == Qt.LeftButton and self.drag_position is not None:
            self.move(event.globalPosition().toPoint() - self.drag_position)
            event.accept()

    def mouseReleaseEvent(self, event):
        if event.button() == Qt.LeftButton:
            self.drag_position = None
            event.accept()

# ============================================================
# MAIN
# ============================================================
def main():
    """Main entry point with comprehensive error handling"""
    log("="*60, "INFO")
    log("KOKORO AI COMPANION STARTING", "INFO")
    log("="*60, "INFO")
    
    # Signal handlers
    def signal_handler(signum, frame):
        log(f"Signal {signum} received, shutting down...", "INFO")
        QCoreApplication.quit()
    
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    # Environment
    os.environ["QT_AUTO_SCREEN_SCALE_FACTOR"] = "1"
    os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"

    exit_code = 0
    
    try:
        log("Creating QApplication")
        app = QApplication(sys.argv)
        
        # Set global OpenGL format for transparency
        fmt = QSurfaceFormat()
        fmt.setAlphaBufferSize(8)
        QSurfaceFormat.setDefaultFormat(fmt)

        # Initialize TTS pipeline
        log("Initializing TTS pipeline...")
        pipeline = KPipeline(lang_code="a", device="cpu")
        log("TTS pipeline ready")

        # Initialize Live2D
        log("Initializing Live2D framework...")
        live2d.init()
        log("Live2D ready")

        # Create widget
        log("Creating Live2D widget...")
        widget = Live2DWidget()
        widget.set_pipeline(pipeline)
        widget.show()
        log("Widget displayed")

        # Start input thread
        log("Starting input thread...")
        input_thread = InputThread()
        input_thread.text_received.connect(widget.enqueue_tts)
        input_thread.error.connect(widget.on_error)
        input_thread.start()
        log("Input thread started")

        log("="*60, "INFO")
        log("READY - System operational", "INFO")
        log("="*60, "INFO")

        # Run event loop
        exit_code = app.exec()
        log(f"Event loop exited with code: {exit_code}", "INFO")
        
    except Exception as e:
        log(f"Fatal error in main: {e}", "ERROR")
        traceback.print_exc()
        exit_code = 1
        
    finally:
        log("Cleanup starting...", "INFO")
        
        try:
            if 'input_thread' in locals():
                input_thread.stop()
                if not input_thread.wait(3000):
                    log("Input thread did not stop in time", "WARNING")
        except:
            pass
        
        try:
            live2d.dispose()
        except:
            pass
        
        log("="*60, "INFO")
        log("KOKORO AI COMPANION STOPPED", "INFO")
        log("="*60, "INFO")
        
    sys.exit(exit_code)

if __name__ == "__main__":
    main()

##### FILE: ./tests/tts.py #####

import time
import torch
import numpy as np
import sounddevice as sd
from kokoro import KPipeline
import os

os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"

# -----------------------------
# Device selection
# -----------------------------
# if torch.backends.mps.is_available():
#     device = "mps"
# elif torch.cuda.is_available():
#     device = "cuda"
# else:
#     device = "cpu"

device = "cpu"

print(f"Using device: {device}")

# -----------------------------
# Initialize pipeline ONCE
# -----------------------------
print("Initializing Kokoro pipeline...")
pipeline = KPipeline(lang_code="a", device=device)
print("Pipeline ready.\n")

SAMPLE_RATE = 22050

# -----------------------------
# Continuous TTS loop
# -----------------------------
while True:
    try:
        text = input("\nEnter text (or empty to quit): ").strip()
        if not text:
            print("Exiting.")
            break

        print("Starting inference timer...")
        t0 = time.perf_counter()

        gen = pipeline(text, voice="af_aoede")

        audio_chunks = []
        for r in gen:
            audio_chunks.append(
                r.output.audio.detach().cpu().numpy()
            )

        audio = np.concatenate(audio_chunks)

        t1 = time.perf_counter()
        latency = t1 - t0

        print(f"Inference complete.")
        print(f"Latency: {latency:.3f} sec")
        print(f"Audio duration: {len(audio) / SAMPLE_RATE:.2f} sec")

        print("Playing audio...")
        sd.play(audio, samplerate=SAMPLE_RATE)
        sd.wait()

    except KeyboardInterrupt:
        print("\nInterrupted. Exiting.")
        break


##### FILE: ./tests/mem_chat.py #####

# mem_chat.py
import asyncio
import logging

from llama_cpp import Llama

from graphiti_core import Graphiti
from graphiti_core.llm_client.config import LLMConfig
from graphiti_core.llm_client.openai_generic_client import OpenAIGenericClient
from graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig
from graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient

# ======================================================
# CONFIG
# ======================================================

NEO4J_URI = "bolt://localhost:7687"
NEO4J_USER = "neo4j"
NEO4J_PASSWORD = "password"

OLLAMA_BASE_URL = "http://localhost:11434/v1"
OLLAMA_LLM_MODEL = "llama3.1:8b"
OLLAMA_EMBED_MODEL = "nomic-embed-text"

GGUF_MODEL_PATH = "/Users/teoi/Documents/ene/models/Meta-Llama-3-8B-Instruct.Q4_1.gguf"

MAX_MEMORY_FACTS = 5

# ======================================================
# LOGGING
# ======================================================

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(message)s",
)
log = logging.getLogger("ene.chat")

# ======================================================
# SYSTEM PROMPT
# ======================================================

SYSTEM_PROMPT = """
You are Ene.

Rules:
- You may only state facts that appear in the provided memory facts.
- If the memory facts do not contain the answer, say: "I don't know yet."
- Do NOT guess or infer missing information.
- You may be playful in tone, but not in facts.
""".strip()

# ======================================================
# GGUF MODEL
# ======================================================

llm = Llama(
    model_path=GGUF_MODEL_PATH,
    n_ctx=4096,
    n_threads=8,
    verbose=False,
)

# ======================================================
# PROMPT BUILDER
# ======================================================

def build_prompt(user_input: str, memories: list[str]) -> str:
    memory_block = ""
    if memories:
        memory_block = "Memory facts:\n" + "\n".join(f"- {m}" for m in memories)

    return f"""{SYSTEM_PROMPT}

{memory_block}

User: {user_input}
Ene:"""

# ======================================================
# MAIN
# ======================================================

async def main():
    log.info("üß± Initializing Graphiti (chat mode)")

    llm_config = LLMConfig(
        api_key="ollama",
        model=OLLAMA_LLM_MODEL,
        small_model=OLLAMA_LLM_MODEL,
        base_url=OLLAMA_BASE_URL,
    )

    llm_client = OpenAIGenericClient(config=llm_config)

    embedder = OpenAIEmbedder(
        config=OpenAIEmbedderConfig(
            api_key="ollama",
            embedding_model=OLLAMA_EMBED_MODEL,
            embedding_dim=768,
            base_url=OLLAMA_BASE_URL,
        )
    )

    reranker = OpenAIRerankerClient(
        client=llm_client,
        config=llm_config,
    )

    graphiti = Graphiti(
        NEO4J_URI,
        NEO4J_USER,
        NEO4J_PASSWORD,
        llm_client=llm_client,
        embedder=embedder,
        cross_encoder=reranker,
    )

    try:
        print("\nüß† Ene Memory Chat (type 'exit')\n")

        while True:
            user_input = input("You: ").strip()
            if user_input.lower() in {"exit", "quit"}:
                break

            log.info(f"üîç Querying memory graph for: {user_input!r}")
            if "birthday" in user_input.lower() or "born" in user_input.lower():
                recall_query = "Teo born birthday date"
            else:
                recall_query = user_input

            results = await graphiti.search(recall_query)

            memories = [r.fact for r in results[:MAX_MEMORY_FACTS]]

            if memories:
                log.info("üß† Retrieved memory facts:")
                for i, m in enumerate(memories, 1):
                    log.info(f"   [{i}] {m}")
            else:
                log.info("üß† No relevant memories found")

            prompt = build_prompt(user_input, memories)

            output = llm(
                prompt,
                max_tokens=256,
                temperature=0.6,
                stop=["User:"],
            )

            response = output["choices"][0]["text"].strip()
            print(f"\nEne: {response}\n")

    finally:
        await graphiti.close()
        log.info("üîí Graphiti connection closed")


if __name__ == "__main__":
    asyncio.run(main())


##### FILE: ./tests/chat.py #####

import time
from llama_cpp import Llama
from pathlib import Path

MODEL_PATH = "models/qwen2.5-3b-instruct-q4_k_m.gguf"

llm = Llama(
    model_path=MODEL_PATH,
    n_ctx=32768,
    n_gpu_layers=-1,
    n_threads=8,
    verbose=False,
)

SYSTEM_PROMPT = Path("system_prompt.txt").read_text(encoding="utf-8").strip()

def chat_once(user_msg: str):
    prompt = f"""<|im_start|>system
{SYSTEM_PROMPT}
<|im_end|>
<|im_start|>user
{user_msg}
<|im_end|>
<|im_start|>assistant
"""

    start_time = time.perf_counter()
    first_token_time = None
    token_count = 0

    for chunk in llm(
        prompt,
        max_tokens=256,
        temperature=0.5,
        top_p=0.9,
        stop=["<|im_end|>"],
        stream=True,
    ):
        text = chunk["choices"][0]["text"]
        if text:
            if first_token_time is None:
                first_token_time = time.perf_counter()

            token_count += 1
            print(text, end="", flush=True)

    end_time = time.perf_counter()

    total_time = end_time - start_time
    ttft = (first_token_time - start_time) if first_token_time else 0
    tps = token_count / (end_time - first_token_time) if first_token_time else 0

    print("\n")
    print(f"‚è± TTFT: {ttft:.3f}s | üßÆ Tokens: {token_count} | ‚ö° {tps:.1f} tok/s | ‚åõ Total: {total_time:.2f}s")
    print()

if __name__ == "__main__":
    print("Qwen2.5 local chat test with timing (type 'exit' to quit)\n")

    while True:
        user_input = input("You: ")
        if user_input.lower() in {"exit", "quit"}:
            break

        print("Assistant: ", end="", flush=True)
        chat_once(user_input)

##### FILE: ./tests/mem.py #####

# mem.py
import asyncio
import logging
from datetime import datetime, timezone

from graphiti_core import Graphiti
from graphiti_core.nodes import EpisodeType
from graphiti_core.llm_client.config import LLMConfig
from graphiti_core.llm_client.openai_generic_client import OpenAIGenericClient
from graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig
from graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient

# ======================================================
# CONFIG
# ======================================================

NEO4J_URI = "bolt://localhost:7687"
NEO4J_USER = "neo4j"
NEO4J_PASSWORD = "password"

OLLAMA_BASE_URL = "http://localhost:11434/v1"
OLLAMA_LLM_MODEL = "llama3.1:8b"
OLLAMA_EMBED_MODEL = "nomic-embed-text"

# ======================================================
# LOGGING
# ======================================================

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(message)s",
)
log = logging.getLogger("ene.mem")

# ======================================================
# MAIN
# ======================================================

async def main():
    log.info("üß± Initializing Graphiti (memory builder)")

    llm_config = LLMConfig(
        api_key="ollama",
        model=OLLAMA_LLM_MODEL,
        small_model=OLLAMA_LLM_MODEL,
        base_url=OLLAMA_BASE_URL,
    )

    llm_client = OpenAIGenericClient(config=llm_config)

    embedder = OpenAIEmbedder(
        config=OpenAIEmbedderConfig(
            api_key="ollama",
            embedding_model=OLLAMA_EMBED_MODEL,
            embedding_dim=768,
            base_url=OLLAMA_BASE_URL,
        )
    )

    reranker = OpenAIRerankerClient(
        client=llm_client,
        config=llm_config,
    )

    graphiti = Graphiti(
        NEO4J_URI,
        NEO4J_USER,
        NEO4J_PASSWORD,
        llm_client=llm_client,
        embedder=embedder,
        cross_encoder=reranker,
    )

    try:
        # --------------------------------------------------
        # REQUIRED SETUP
        # --------------------------------------------------
        await graphiti.build_indices_and_constraints()

        # --------------------------------------------------
        # MEMORY EPISODES
        # --------------------------------------------------
        episodes = [
            {
                "name": "teo-identity",
                "content": "Teo Imoto-Tar is a person.",
            },
            {
                "name": "teo-birth",
                "content": "Teo Imoto-Tar was born on February 15, 2005.",
            },
            {
                "name": "teo-education",
                "content": (
                    "Teo Imoto-Tar is an undergraduate student studying "
                    "computer science and mathematics at UC San Diego."
                ),
            },
            {
                "name": "teo-work",
                "content": (
                    "Teo Imoto-Tar works as a research assistant at the "
                    "Neuroelectronics Lab in the Jacobs School of Engineering."
                ),
            },
            {
                "name": "teo-interests",
                "content": (
                    "Teo Imoto-Tar is interested in software engineering, "
                    "AI research, computer vision, and computational neuroscience."
                ),
            },
            {
                "name": "teo-hobbies",
                "content": (
                    "Outside of research, Teo Imoto-Tar makes music and likes capybaras."
                ),
            },
            {
                "name": "ene-creation",
                "content": "Ene is an AI assistant created by Teo Imoto-Tar on January 14, 2026.",
            },
        ]

        for i, ep in enumerate(episodes):
            log.info(f"üß† Adding episode: {ep['name']}")
            await graphiti.add_episode(
                name=ep["name"],
                episode_body=ep["content"],
                source=EpisodeType.text,
                source_description="User-defined memory",
                reference_time=datetime.now(timezone.utc),
            )

        log.info("‚úÖ Memory graph populated")

    finally:
        await graphiti.close()
        log.info("üîí Graphiti connection closed")


if __name__ == "__main__":
    asyncio.run(main())


##### FILE: ./logger.py #####

# logger.py
from config import DEBUG

def log(msg: str, level: str = "DEBUG"):
    if DEBUG or level != "DEBUG":
        print(f"[{level}] {msg}", flush=True)


##### FILE: ./tts/audio_worker.py #####

import queue
import traceback
import numpy as np
import sounddevice as sd

from PySide6.QtCore import QThread, Signal, QMutex, QMutexLocker

from config import SAMPLE_RATE, FRAME_SIZE
from logger import log

class AudioWorker(QThread):
    """Thread-safe audio playback with RMS tracking"""
    volume_update = Signal(float)
    error = Signal(str)

    def __init__(self):
        super().__init__()
        self.running = True
        self.audio_queue = queue.Queue()
        self._mutex = QMutex()
        log("AudioWorker initialized")

    def add_audio(self, audio: np.ndarray):
        """Thread-safe audio queueing"""
        try:
            if audio.dtype != np.float32:
                audio = audio.astype(np.float32)
            
            self.audio_queue.put(audio)
            log(f"Audio queued: {len(audio)} samples (queue size: {self.audio_queue.qsize()})")
        except Exception as e:
            log(f"Error queueing audio: {e}", "ERROR")

    def stop(self):
        """Thread-safe stop"""
        with QMutexLocker(self._mutex):
            log("AudioWorker stop requested")
            self.running = False

    def is_running_safe(self) -> bool:
        """Thread-safe running check"""
        with QMutexLocker(self._mutex):
            return self.running

    def run(self):
        log("AudioWorker started")
        frame_count = 0
        
        try:
            with sd.OutputStream(
                samplerate=SAMPLE_RATE,
                channels=1,
                dtype="float32",
                blocksize=FRAME_SIZE
            ) as stream:
                log(f"Audio stream opened (SR={SAMPLE_RATE}, BS={FRAME_SIZE})")

                while self.is_running_safe():
                    try:
                        audio = self.audio_queue.get(timeout=0.05)
                        log(f"Processing audio: {len(audio)} samples")

                        # Process in blocks
                        for i in range(0, len(audio), FRAME_SIZE):
                            if not self.is_running_safe():
                                log("AudioWorker stopped during playback")
                                break

                            frame = audio[i:i + FRAME_SIZE]
                            if len(frame) == 0:
                                continue

                            # Remove DC offset
                            frame = frame - np.mean(frame)
                            
                            # Play audio
                            stream.write(frame.reshape(-1, 1))

                            # Calculate RMS
                            rms = float(np.sqrt(np.mean(frame ** 2)))
                            
                            frame_count += 1
                            if frame_count % 100 == 0:
                                log(f"Frame {frame_count}, RMS: {rms:.6f}")

                            self.volume_update.emit(rms)

                    except queue.Empty:
                        # No audio, emit silence
                        self.volume_update.emit(0.0)

        except Exception as e:
            error_msg = f"AudioWorker error: {e}"
            log(error_msg, "ERROR")
            traceback.print_exc()
            self.error.emit(error_msg)

        log("AudioWorker exited")

    def stop_and_wait(self):
        """Safely stop and wait for thread"""
        self.stop()
        if not self.wait(5000):  # 5 second timeout
            log("AudioWorker did not stop in time", "WARNING")



##### FILE: ./tts/__init__.py #####

from .audio_utils import pitch_resample
from .audio_worker import AudioWorker
from .tts_worker import TTSGenerator

__all__ = [
    "pitch_resample",
    "AudioWorker",
    "TTSGenerator",
]



##### FILE: ./tts/audio_utils.py #####

import numpy as np

def pitch_resample(audio: np.ndarray, semitones: float = 1.0) -> np.ndarray:
    factor = 2 ** (semitones / 12)
    new_len = int(len(audio) / factor)
    return np.interp(
        np.linspace(0, len(audio), new_len),
        np.arange(len(audio)),
        audio
    ).astype(np.float32)



##### FILE: ./tts/tts_worker.py #####

# tts/tts_worker.py

from __future__ import annotations

import traceback

import numpy as np

from PySide6.QtCore import (
    QThread,
    Signal,
    QMutex,
    QMutexLocker,
)

from config import (
    TTS_VOICE
)

from tts.audio_utils import pitch_resample

from logger import log

class TTSGenerator(QThread):
    """Thread-safe TTS generation"""
    audio_chunk = Signal(np.ndarray)
    finished_tts = Signal()
    error = Signal(str)

    def __init__(self, pipeline: KPipeline, text: str):
        super().__init__()
        self.pipeline = pipeline
        self.text = text
        self._mutex = QMutex()
        self._running = True
        log(f"TTSGenerator created: '{text}'")

    def stop(self):
        """Thread-safe stop"""
        with QMutexLocker(self._mutex):
            self._running = False

    def is_running_safe(self) -> bool:
        """Thread-safe running check"""
        with QMutexLocker(self._mutex):
            return self._running

    def run(self):
        log(f"TTSGenerator started: '{self.text}'")
        chunk_count = 0
        
        try:
            for r in self.pipeline(self.text, voice=TTS_VOICE):
                if not self.is_running_safe():
                    log("TTSGenerator stopped early")
                    break
                
                chunk_count += 1
                audio = (
                    r.output.audio
                    .detach()
                    .cpu()
                    .numpy()
                    .astype(np.float32)
                )

                audio = pitch_resample(audio, semitones=4.0)  

                # Normalize
                audio = audio - np.mean(audio)
                np.clip(audio, -1.0, 1.0, out=audio)

                log(f"TTS chunk #{chunk_count}: {len(audio)} samples, range=[{audio.min():.3f}, {audio.max():.3f}]")
                self.audio_chunk.emit(audio)

            log(f"TTSGenerator completed ({chunk_count} chunks)")
            
        except Exception as e:
            error_msg = f"TTS error: {e}"
            log(error_msg, "ERROR")
            traceback.print_exc()
            self.error.emit(error_msg)
        finally:
            self.finished_tts.emit()



##### FILE: ./live2d_ui/model_utils.py #####

import json
import traceback
from pathlib import Path

def get_fixed_model_path(original_path: Path) -> Path:
    try:
        fixed_path = original_path.with_stem(original_path.stem + "_fixed")
        if fixed_path.exists():
            return fixed_path

        with open(original_path, "r", encoding="utf-8") as f:
            data = json.load(f)

        data.pop("DefaultExpression", None)
        if "FileReferences" in data:
            data["FileReferences"].pop("DefaultExpression", None)

        with open(fixed_path, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=4)

        return fixed_path
    except Exception:
        traceback.print_exc()
        return original_path



##### FILE: ./live2d_ui/widget.py #####

import sys
import time
import math
import queue
import traceback
from pathlib import Path

from kokoro import KPipeline

from PySide6.QtOpenGLWidgets import QOpenGLWidget
from PySide6.QtCore import Qt, QTimer, QMutex, QMutexLocker
from PySide6.QtGui import QSurfaceFormat, QGuiApplication

from OpenGL.GL import *

import live2d.v3 as live2d  

from logger import log
from config import (
    WINDOW_WIDTH,
    WINDOW_HEIGHT,
    FPS,
    LIP_SYNC_SENSITIVITY,
    LIP_SYNC_SMOOTHING,
    LIP_SYNC_THRESHOLD,
    AUDIO_NOISE_FLOOR,
    AUDIO_VOLUME_GAIN,
    AUDIO_COMPRESSION,
    SAMPLE_RATE,
    FRAME_SIZE,
    LIVE2D_MODEL_PATH,
)

from tts import AudioWorker, TTSGenerator
from llm import LLMWorker
from .model_utils import get_fixed_model_path

class Live2DWidget(QOpenGLWidget):
    """Main widget with Live2D rendering and audio/TTS coordination"""
    
    def __init__(self, llm, system_prompt, parent=None):
        super().__init__(parent)

        self.llm = llm
        self.system_prompt = system_prompt
        log("Live2DWidget initializing")

        # State
        self.model = None
        self.start_time = time.time()
        self.pipeline = None
        
        # Audio/mouth state
        self.mouth_raw = 0.0
        self.mouth_value = 0.0
        self._peak = 1e-6
        
        # TTS queue
        self.tts_queue = queue.Queue()
        self.tts_busy = False
        self._tts_start_time = None
        self._tts_mutex = QMutex()
        
        # First frame ghost fix
        self._first_frame_rendered = False
        self._ghost_fix_timer = None
        
        # Workers
        self.audio_worker = AudioWorker()
        self.audio_worker.volume_update.connect(self.on_volume)
        self.audio_worker.error.connect(self.on_error)
        
        # OpenGL format
        fmt = QSurfaceFormat()
        fmt.setRenderableType(QSurfaceFormat.OpenGL)
        fmt.setVersion(2, 1)
        fmt.setProfile(QSurfaceFormat.NoProfile)
        fmt.setAlphaBufferSize(8)
        self.setFormat(fmt)
        
        # Window setup
        flags = (
            Qt.FramelessWindowHint | 
            Qt.WindowStaysOnTopHint | 
            Qt.Window |
            Qt.BypassWindowManagerHint
        )
        self.setWindowFlags(flags)
        self.setFixedSize(WINDOW_WIDTH, WINDOW_HEIGHT)
        
        # Transparency attributes
        self.setAttribute(Qt.WA_TranslucentBackground)
        self.setAttribute(Qt.WA_NoSystemBackground)
        self.setAttribute(Qt.WA_OpaquePaintEvent, False)
        self.setAutoFillBackground(False)
        self.setUpdateBehavior(QOpenGLWidget.NoPartialUpdate)
        
        self.setMouseTracking(True)
        self.drag_position = None

        # Update timer
        self.timer = QTimer(self)
        self.timer.timeout.connect(self.update)
        self.timer.start(1000 // FPS)
        
        # macOS ghosting fix
        if sys.platform == "darwin":
            QGuiApplication.instance().applicationStateChanged.connect(self._on_app_state_changed)
        
        log(f"Live2DWidget initialized ({FPS} FPS)")

    def on_error(self, error_msg: str):
        """Handle errors from worker threads"""
        log(f"Widget received error: {error_msg}", "ERROR")

    def set_pipeline(self, pipeline: KPipeline):
        """Set TTS pipeline"""
        log("TTS pipeline set")
        self.pipeline = pipeline

    def enqueue_tts(self, text: str):
        """Handle new user input"""
        log(f"\n{'='*60}")
        log(f"New input: '{text}'")
        log('='*60)
        
        try:
            # Stop existing LLM worker
            if hasattr(self, "llm_worker") and self.llm_worker.isRunning():
                log("Stopping previous LLM worker")
                self.llm_worker.stop()
                if not self.llm_worker.wait(3000):
                    log("LLM worker did not stop in time", "WARNING")

            # Stop existing TTS
            if hasattr(self, "current_tts") and self.current_tts.isRunning():
                log("Stopping current TTS")
                self.current_tts.stop()
                if not self.current_tts.wait(3000):
                    log("TTS did not stop in time", "WARNING")
            
            # Clear TTS queue
            cleared = 0
            while not self.tts_queue.empty():
                try:
                    self.tts_queue.get_nowait()
                    cleared += 1
                except queue.Empty:
                    break
            
            if cleared > 0:
                log(f"Cleared {cleared} items from TTS queue")
            
            with QMutexLocker(self._tts_mutex):
                self.tts_busy = False

            # Start new LLM worker
            self.llm_worker = LLMWorker(
                self.llm,
                self.system_prompt,
                text,
            )
            self.llm_worker.text_chunk.connect(self.on_llm_chunk)
            self.llm_worker.finished.connect(self.on_llm_finished)
            self.llm_worker.error.connect(self.on_error)
            self.llm_worker.start()
            
            log("LLM worker started")
            
        except Exception as e:
            log(f"Error in enqueue_tts: {e}", "ERROR")
            traceback.print_exc()

    def on_llm_chunk(self, text: str):
        """Handle LLM chunk - add to TTS queue"""
        try:
            self.tts_queue.put(text)
            log(f"Added to TTS queue (size: {self.tts_queue.qsize()})")
            self.process_queue()
        except Exception as e:
            log(f"Error handling LLM chunk: {e}", "ERROR")

    def on_llm_finished(self):
        """Handle LLM completion"""
        log("LLM generation finished")
        self.process_queue()

    def process_queue(self):
        """Process next item in TTS queue"""
        with QMutexLocker(self._tts_mutex):
            is_busy = self.tts_busy
            queue_size = self.tts_queue.qsize()
        
        log(f"process_queue: busy={is_busy}, queue_size={queue_size}")
        
        if is_busy or queue_size == 0:
            return  
        
        try:
            # Stop any running TTS first
            if hasattr(self, "current_tts") and self.current_tts.isRunning():
                log("Stopping previous TTS")
                self.current_tts.stop()
                if not self.current_tts.wait(2000):
                    log("Previous TTS did not stop in time", "WARNING")

            with QMutexLocker(self._tts_mutex):
                self.tts_busy = True
            
            text = self.tts_queue.get()
            log(f"Starting TTS: '{text}'")

            if not self.pipeline:
                log("No TTS pipeline available", "ERROR")
                with QMutexLocker(self._tts_mutex):
                    self.tts_busy = False
                return

            self.current_tts = TTSGenerator(self.pipeline, text)
            self.current_tts.audio_chunk.connect(self.audio_worker.add_audio)
            self.current_tts.finished_tts.connect(self.on_tts_done)
            self.current_tts.error.connect(self.on_error)

            self._tts_start_time = time.time()
            self.current_tts.start()
            
        except Exception as e:
            log(f"Error in process_queue: {e}", "ERROR")
            traceback.print_exc()
            with QMutexLocker(self._tts_mutex):
                self.tts_busy = False

    def on_tts_done(self):
        """Handle TTS completion"""
        try:
            if self._tts_start_time:
                elapsed = time.time() - self._tts_start_time
                log(f"TTS completed in {elapsed:.2f}s", "INFO")

            self._tts_start_time = None
            
            with QMutexLocker(self._tts_mutex):
                self.tts_busy = False
            
            self.process_queue()
            
        except Exception as e:
            log(f"Error in on_tts_done: {e}", "ERROR")
            traceback.print_exc()

    def initializeGL(self):
        """Initialize OpenGL and Live2D"""
        try:
            log("Initializing OpenGL")
            live2d.glInit()
            glClearColor(0, 0, 0, 0)
            glEnable(GL_BLEND)
            glBlendFunc(GL_SRC_ALPHA, GL_ONE_MINUS_SRC_ALPHA)

            path = get_fixed_model_path(Path(LIVE2D_MODEL_PATH))
            log(f"Loading Live2D model: {path}")

            self.model = live2d.LAppModel()
            self.model.LoadModelJson(str(path))
            self.model.Resize(self.width(), self.height())
            
            log(f"Live2D model loaded ({self.width()}x{self.height()})")

            self.audio_worker.start()
            log("Audio worker started")
            
        except Exception as e:
            log(f"Error in initializeGL: {e}", "ERROR")
            traceback.print_exc()

    def resizeGL(self, w: int, h: int):
        """Handle window resizing"""
        if self.model:
            self.model.Resize(w, h)

    def on_volume(self, v_raw: float):
        """Process audio volume for lip sync"""
        try:
            # Floor noise
            v = max(0.0, v_raw - AUDIO_NOISE_FLOOR)

            # Compress and gain
            v = v * AUDIO_VOLUME_GAIN
            v = v ** AUDIO_COMPRESSION

            # Track peak
            self._peak = max(self._peak * 0.995, max(v, 1e-6))

            # Normalize
            v = v / (self._peak + 1e-6)
            v = min(max(v * LIP_SYNC_SENSITIVITY, 0.0), 1.0)

            # Smooth
            self.mouth_raw = max(self.mouth_raw * 0.85, v)
            
        except Exception as e:
            log(f"Error in on_volume: {e}", "ERROR")

    def _apply_first_frame_fix(self):
        """Restore model to normal size after initial ghost frame"""
        try:
            if self.model:
                # Reset to normal scale
                self.model.SetParameterValue("ParamBodyScaleX", 1.0)
                self.model.SetParameterValue("ParamBodyScaleY", 1.0)
                self._first_frame_rendered = True
                log("First frame ghost fix applied - model restored to normal size", "INFO")
        except Exception as e:
            log(f"Error in first frame fix: {e}", "ERROR")

    def paintGL(self):
        """Render Live2D model"""
        try:
            live2d.clearBuffer()
            if not self.model:
                return

            # First frame ghost fix for macOS Metal
            if not self._first_frame_rendered and sys.platform == "darwin":
                log("Rendering first frame at tiny scale to prevent ghost", "INFO")
                # Scale model to nearly invisible for first frame
                self.model.SetParameterValue("ParamBodyScaleX", 0.001)
                self.model.SetParameterValue("ParamBodyScaleY", 0.001)
                
                # Schedule restoration to normal size
                if not self._ghost_fix_timer:
                    self._ghost_fix_timer = QTimer(self)
                    self._ghost_fix_timer.setSingleShot(True)
                    self._ghost_fix_timer.timeout.connect(self._apply_first_frame_fix)
                    self._ghost_fix_timer.start(100)  # 100ms delay

            # Breathing animation
            t = time.time() - self.start_time
            self.model.SetParameterValue("PARAM_BREATH", (math.sin(t * 2) + 1) / 2)

            # Mouth animation
            target = min(self.mouth_raw, 1.0) if self.mouth_raw > LIP_SYNC_THRESHOLD else 0.0
            self.mouth_value += (target - self.mouth_value) * LIP_SYNC_SMOOTHING

            for p in ("PARAM_MOUTH_OPEN_Y", "ParamMouthOpenY"):
                self.model.SetParameterValue(p, self.mouth_value)

            self.model.Update()
            self.model.Draw()
            
        except Exception as e:
            log(f"Error in paintGL: {e}", "ERROR")

    def _on_app_state_changed(self, state):
        """Forces a redraw on app activation to fix ghosting on macOS"""
        if state != Qt.ApplicationActive or not self.isValid():
            return

        try:
            self.makeCurrent()
            glFinish()

            w, h = self.width(), self.height()
            super().resizeGL(w + 1, h + 1)
            super().resizeGL(w, h)

            for _ in range(2):
                glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT)
                live2d.clearBuffer()
                self.update()
                
            log("macOS ghost fix applied (app state changed)", "INFO")
        except Exception as e:
            log(f"Error in _on_app_state_changed: {e}", "ERROR")

    def showEvent(self, event):
        """Ensure first frame draws correctly"""
        super().showEvent(event)
        QTimer.singleShot(0, self.update)

    def closeEvent(self, event):
        """Clean shutdown"""
        log("Shutting down Live2DWidget")
        
        try:
            # Stop ghost fix timer
            if self._ghost_fix_timer:
                self._ghost_fix_timer.stop()
            
            # Stop LLM worker
            if hasattr(self, "llm_worker") and self.llm_worker.isRunning():
                log("Stopping LLM worker")
                self.llm_worker.stop()
                self.llm_worker.wait(3000)

            # Stop TTS
            if hasattr(self, "current_tts") and self.current_tts.isRunning():
                log("Stopping TTS")
                self.current_tts.stop()
                self.current_tts.wait(3000)

            # Stop audio worker
            log("Stopping audio worker")
            self.audio_worker.stop_and_wait()
            
            event.accept()
            log("Shutdown complete")
            
        except Exception as e:
            log(f"Error during shutdown: {e}", "ERROR")
            traceback.print_exc()
            event.accept()

    # Mouse event handlers for dragging
    def mousePressEvent(self, event):
        if event.button() == Qt.LeftButton:
            self.drag_position = (
                event.globalPosition().toPoint() - self.frameGeometry().topLeft()
            )
            event.accept()

    def mouseMoveEvent(self, event):
        if event.buttons() == Qt.LeftButton and self.drag_position is not None:
            self.move(event.globalPosition().toPoint() - self.drag_position)
            event.accept()

    def mouseReleaseEvent(self, event):
        if event.button() == Qt.LeftButton:
            self.drag_position = None
            event.accept()


##### FILE: ./live2d_ui/__init__.py #####

from .widget import Live2DWidget
from .model_utils import get_fixed_model_path

__all__ = [
    "Live2DWidget",
    "get_fixed_model_path",
]


##### FILE: ./main.py #####

# main.py
import sys
import os
from PySide6.QtWidgets import QApplication
import live2d.v3 as live2d

from llm.model import init_llm, load_system_prompt
from live2d_ui import Live2DWidget
from input.input_thread import InputThread
from kokoro import KPipeline

def main():
    os.environ["QT_AUTO_SCREEN_SCALE_FACTOR"] = "1"
    os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"

    app = QApplication(sys.argv)

    live2d.init()

    llm = init_llm()
    system_prompt = load_system_prompt()

    pipeline = KPipeline(lang_code="a", device="mps")

    widget = Live2DWidget(llm, system_prompt)
    widget.set_pipeline(pipeline)
    widget.show()

    input_thread = InputThread()
    input_thread.text_received.connect(widget.enqueue_tts)
    input_thread.start()

    sys.exit(app.exec())

if __name__ == "__main__":
    main()



##### FILE: ./kokoro/demo/app.py #####

import spaces
from kokoro import KModel, KPipeline
import gradio as gr
import os
import random
import torch

CUDA_AVAILABLE = torch.cuda.is_available()
models = {gpu: KModel().to('cuda' if gpu else 'cpu').eval() for gpu in [False] + ([True] if CUDA_AVAILABLE else [])}
pipelines = {lang_code: KPipeline(lang_code=lang_code, model=False) for lang_code in 'ab'}
pipelines['a'].g2p.lexicon.golds['kokoro'] = 'kÀàOk…ô…πO'
pipelines['b'].g2p.lexicon.golds['kokoro'] = 'kÀàQk…ô…πQ'

@spaces.GPU(duration=30)
def forward_gpu(ps, ref_s, speed):
    return models[True](ps, ref_s, speed)

def generate_first(text, voice='af_heart', speed=1, use_gpu=CUDA_AVAILABLE):
    pipeline = pipelines[voice[0]]
    pack = pipeline.load_voice(voice)
    use_gpu = use_gpu and CUDA_AVAILABLE
    for _, ps, _ in pipeline(text, voice, speed):
        ref_s = pack[len(ps)-1]
        try:
            if use_gpu:
                audio = forward_gpu(ps, ref_s, speed)
            else:
                audio = models[False](ps, ref_s, speed)
        except gr.exceptions.Error as e:
            if use_gpu:
                gr.Warning(str(e))
                gr.Info('Retrying with CPU. To avoid this error, change Hardware to CPU.')
                audio = models[False](ps, ref_s, speed)
            else:
                raise gr.Error(e)
        return (24000, audio.numpy()), ps
    return None, ''

# Arena API
def predict(text, voice='af_heart', speed=1):
    return generate_first(text, voice, speed, use_gpu=False)[0]

def tokenize_first(text, voice='af_heart'):
    pipeline = pipelines[voice[0]]
    for _, ps, _ in pipeline(text, voice):
        return ps
    return ''

def generate_all(text, voice='af_heart', speed=1, use_gpu=CUDA_AVAILABLE):
    pipeline = pipelines[voice[0]]
    pack = pipeline.load_voice(voice)
    use_gpu = use_gpu and CUDA_AVAILABLE
    first = True
    for _, ps, _ in pipeline(text, voice, speed):
        ref_s = pack[len(ps)-1]
        try:
            if use_gpu:
                audio = forward_gpu(ps, ref_s, speed)
            else:
                audio = models[False](ps, ref_s, speed)
        except gr.exceptions.Error as e:
            if use_gpu:
                gr.Warning(str(e))
                gr.Info('Switching to CPU')
                audio = models[False](ps, ref_s, speed)
            else:
                raise gr.Error(e)
        yield 24000, audio.numpy()
        if first:
            first = False
            yield 24000, torch.zeros(1).numpy()

with open('en.txt', 'r') as r:
    random_quotes = [line.strip() for line in r]

def get_random_quote():
    return random.choice(random_quotes)

def get_gatsby():
    with open('gatsby5k.md', 'r') as r:
        return r.read().strip()

def get_frankenstein():
    with open('frankenstein5k.md', 'r') as r:
        return r.read().strip()

CHOICES = {
'üá∫üá∏ üö∫ Heart ‚ù§Ô∏è': 'af_heart',
'üá∫üá∏ üö∫ Bella üî•': 'af_bella',
'üá∫üá∏ üö∫ Nicole üéß': 'af_nicole',
'üá∫üá∏ üö∫ Aoede': 'af_aoede',
'üá∫üá∏ üö∫ Kore': 'af_kore',
'üá∫üá∏ üö∫ Sarah': 'af_sarah',
'üá∫üá∏ üö∫ Nova': 'af_nova',
'üá∫üá∏ üö∫ Sky': 'af_sky',
'üá∫üá∏ üö∫ Alloy': 'af_alloy',
'üá∫üá∏ üö∫ Jessica': 'af_jessica',
'üá∫üá∏ üö∫ River': 'af_river',
'üá∫üá∏ üöπ Michael': 'am_michael',
'üá∫üá∏ üöπ Fenrir': 'am_fenrir',
'üá∫üá∏ üöπ Puck': 'am_puck',
'üá∫üá∏ üöπ Echo': 'am_echo',
'üá∫üá∏ üöπ Eric': 'am_eric',
'üá∫üá∏ üöπ Liam': 'am_liam',
'üá∫üá∏ üöπ Onyx': 'am_onyx',
'üá∫üá∏ üöπ Santa': 'am_santa',
'üá∫üá∏ üöπ Adam': 'am_adam',
'üá¨üáß üö∫ Emma': 'bf_emma',
'üá¨üáß üö∫ Isabella': 'bf_isabella',
'üá¨üáß üö∫ Alice': 'bf_alice',
'üá¨üáß üö∫ Lily': 'bf_lily',
'üá¨üáß üöπ George': 'bm_george',
'üá¨üáß üöπ Fable': 'bm_fable',
'üá¨üáß üöπ Lewis': 'bm_lewis',
'üá¨üáß üöπ Daniel': 'bm_daniel',
}
for v in CHOICES.values():
    pipelines[v[0]].load_voice(v)

TOKEN_NOTE = '''
üí° Customize pronunciation with Markdown link syntax and /slashes/ like `[Kokoro](/kÀàOk…ô…πO/)`

üí¨ To adjust intonation, try punctuation `;:,.!?‚Äî‚Ä¶"()‚Äú‚Äù` or stress `Àà` and `Àå`

‚¨áÔ∏è Lower stress `[1 level](-1)` or `[2 levels](-2)`

‚¨ÜÔ∏è Raise stress 1 level `[or](+2)` 2 levels (only works on less stressed, usually short words)
'''

with gr.Blocks() as generate_tab:
    out_audio = gr.Audio(label='Output Audio', interactive=False, streaming=False, autoplay=True)
    generate_btn = gr.Button('Generate', variant='primary')
    with gr.Accordion('Output Tokens', open=True):
        out_ps = gr.Textbox(interactive=False, show_label=False, info='Tokens used to generate the audio, up to 510 context length.')
        tokenize_btn = gr.Button('Tokenize', variant='secondary')
        gr.Markdown(TOKEN_NOTE)
        predict_btn = gr.Button('Predict', variant='secondary', visible=False)

STREAM_NOTE = ['‚ö†Ô∏è There is an unknown Gradio bug that might yield no audio the first time you click `Stream`.']
STREAM_NOTE = '\n\n'.join(STREAM_NOTE)

with gr.Blocks() as stream_tab:
    out_stream = gr.Audio(label='Output Audio Stream', interactive=False, streaming=True, autoplay=True)
    with gr.Row():
        stream_btn = gr.Button('Stream', variant='primary')
        stop_btn = gr.Button('Stop', variant='stop')
    with gr.Accordion('Note', open=True):
        gr.Markdown(STREAM_NOTE)
        gr.DuplicateButton()

API_OPEN = True
with gr.Blocks() as app:
    with gr.Row():
        with gr.Column():
            text = gr.Textbox(label='Input Text', info=f"Arbitrarily many characters supported")
            with gr.Row():
                voice = gr.Dropdown(list(CHOICES.items()), value='af_heart', label='Voice', info='Quality and availability vary by language')
                use_gpu = gr.Dropdown(
                    [('ZeroGPU üöÄ', True), ('CPU üêå', False)],
                    value=CUDA_AVAILABLE,
                    label='Hardware',
                    info='GPU is usually faster, but has a usage quota',
                    interactive=CUDA_AVAILABLE
                )
            speed = gr.Slider(minimum=0.5, maximum=2, value=1, step=0.1, label='Speed')
            random_btn = gr.Button('üé≤ Random Quote üí¨', variant='secondary')
            with gr.Row():
                gatsby_btn = gr.Button('ü•Ç Gatsby üìï', variant='secondary')
                frankenstein_btn = gr.Button('üíÄ Frankenstein üìó', variant='secondary')
        with gr.Column():
            gr.TabbedInterface([generate_tab, stream_tab], ['Generate', 'Stream'])
    random_btn.click(fn=get_random_quote, inputs=[], outputs=[text])
    gatsby_btn.click(fn=get_gatsby, inputs=[], outputs=[text])
    frankenstein_btn.click(fn=get_frankenstein, inputs=[], outputs=[text])
    generate_btn.click(fn=generate_first, inputs=[text, voice, speed, use_gpu], outputs=[out_audio, out_ps])
    tokenize_btn.click(fn=tokenize_first, inputs=[text, voice], outputs=[out_ps])
    stream_event = stream_btn.click(fn=generate_all, inputs=[text, voice, speed, use_gpu], outputs=[out_stream])
    stop_btn.click(fn=None, cancels=stream_event)
    predict_btn.click(fn=predict, inputs=[text, voice, speed], outputs=[out_audio])

if __name__ == '__main__':
    app.queue(api_open=API_OPEN).launch(server_name="0.0.0.0", server_port=40001, show_api=API_OPEN)


##### FILE: ./kokoro/tests/test_custom_stft.py #####

import torch
import numpy as np
import pytest
from kokoro.custom_stft import CustomSTFT
from kokoro.istftnet import TorchSTFT
import torch.nn.functional as F


@pytest.fixture
def sample_audio():
    # Generate a sample audio signal (sine wave)
    sample_rate = 16000
    duration = 1.0  # seconds
    t = torch.linspace(0, duration, int(sample_rate * duration))
    frequency = 440.0  # Hz
    signal = torch.sin(2 * np.pi * frequency * t)
    return signal.unsqueeze(0)  # Add batch dimension


def test_stft_reconstruction(sample_audio):
    # Initialize both STFT implementations
    custom_stft = CustomSTFT(filter_length=800, hop_length=200, win_length=800)
    torch_stft = TorchSTFT(filter_length=800, hop_length=200, win_length=800)

    # Process through both implementations
    custom_output = custom_stft(sample_audio)
    torch_output = torch_stft(sample_audio)

    # Compare outputs
    assert torch.allclose(custom_output, torch_output, rtol=1e-3, atol=1e-3)


def test_magnitude_phase_consistency(sample_audio):
    custom_stft = CustomSTFT(filter_length=800, hop_length=200, win_length=800)
    torch_stft = TorchSTFT(filter_length=800, hop_length=200, win_length=800)

    # Get magnitude and phase from both implementations
    custom_mag, custom_phase = custom_stft.transform(sample_audio)
    torch_mag, torch_phase = torch_stft.transform(sample_audio)

    # Compare magnitudes ignoring the boundary frames
    custom_mag_center = custom_mag[..., 2:-2]
    torch_mag_center = torch_mag[..., 2:-2]
    assert torch.allclose(custom_mag_center, torch_mag_center, rtol=1e-2, atol=1e-2)


def test_batch_processing():
    # Create a batch of signals
    batch_size = 4
    sample_rate = 16000
    duration = 0.1  # shorter duration for faster testing
    t = torch.linspace(0, duration, int(sample_rate * duration))
    frequency = 440.0
    signals = torch.sin(2 * np.pi * frequency * t).unsqueeze(0).repeat(batch_size, 1)

    custom_stft = CustomSTFT(filter_length=800, hop_length=200, win_length=800)

    # Process batch
    output = custom_stft(signals)

    # Check output shape
    assert output.shape[0] == batch_size
    assert len(output.shape) == 3  # (batch, 1, time)


def test_different_window_sizes():
    signal = torch.randn(1, 16000)  # 1 second of random noise

    # Test with different window sizes
    for filter_length in [512, 1024, 2048]:
        custom_stft = CustomSTFT(
            filter_length=filter_length,
            hop_length=filter_length // 4,
            win_length=filter_length,
        )

        # Forward and backward transform
        output = custom_stft(signal)

        # Check that output length is reasonable
        assert output.shape[-1] >= signal.shape[-1]


##### FILE: ./kokoro/examples/phoneme_example.py #####

from kokoro import KPipeline, KModel
import torch
from scipy.io import wavfile

def save_audio(audio: torch.Tensor, filename: str):
    """Helper function to save audio tensor as WAV file"""
    if audio is not None:
        # Ensure audio is on CPU and in the right format
        audio_cpu = audio.cpu().numpy()
        
        # Save using scipy.io.wavfile
        wavfile.write(
            filename,
            24000,  # Kokoro uses 24kHz sample rate
            audio_cpu
        )
        print(f"Audio saved as '{filename}'")
    else:
        print("No audio was generated")

def main():
    # Initialize pipeline with American English
    pipeline = KPipeline(lang_code='a')
    
    # The phoneme string for:
    # "How are you today? I am doing reasonably well, thank you for asking"
    phonemes = "hÀåW …ë…π ju t…ôdÀàA? ÀåI …êm dÀàu…™≈ã …πÀàiz…ôn…ôbli wÀà…õl, Œ∏Àà√¶≈ãk ju f…î…π Àà√¶sk…™≈ã"
    
    try:
        print("\nExample 1: Using generate_from_tokens with raw phonemes")
        results = list(pipeline.generate_from_tokens(
            tokens=phonemes,
            voice="af_bella",
            speed=1.0
        ))
        if results:
            save_audio(results[0].audio, 'phoneme_output_new.wav')
        
        # Example 2: Using generate_from_tokens with pre-processed tokens
        print("\nExample 2: Using generate_from_tokens with pre-processed tokens")
        #  get the tokens through G2P or any other method
        text = "How are you today? I am doing reasonably well, thank you for asking"
        _, tokens = pipeline.g2p(text)
        
        # Then generate from tokens
        for result in pipeline.generate_from_tokens(
            tokens=tokens,
            voice="af_bella",
            speed=1.0
        ):
            # Each result may contain timestamps if available
            if result.tokens:
                for token in result.tokens:
                    if hasattr(token, 'start_ts') and hasattr(token, 'end_ts'):
                        print(f"Token: {token.text} ({token.start_ts:.2f}s - {token.end_ts:.2f}s)")
            save_audio(result.audio, f'token_output_{hash(result.phonemes)}.wav')
            
    except Exception as e:
        print(f"An error occurred: {str(e)}")

if __name__ == "__main__":
    main()

##### FILE: ./kokoro/examples/device_examples.py #####

"""
Quick example to show how device selection can be controlled, and was checked
"""
import time
from kokoro import KPipeline
from loguru import logger

def generate_audio(pipeline, text):
    for _, _, audio in pipeline(text, voice='af_bella'):
        samples = audio.shape[0] if audio is not None else 0
        assert samples > 0, "No audio generated"
        return samples

def time_synthesis(device=None):
    try:
        start = time.perf_counter()
        pipeline = KPipeline(lang_code='a', device=device)
        samples = generate_audio(pipeline, "The quick brown fox jumps over the lazy dog.")
        ms = (time.perf_counter() - start) * 1000
        logger.info(f"‚úì {device or 'auto':<6} | {ms:>5.1f}ms total | {samples:>6,d} samples")
    except RuntimeError as e:
        logger.error(f"‚úó {'cuda' if 'CUDA' in str(e) else device or 'auto':<6} | {'not available' if 'CUDA' in str(e) else str(e)}")

def compare_shared_model():
    try:
        start = time.perf_counter()
        en_us = KPipeline(lang_code='a')
        en_uk = KPipeline(lang_code='a', model=en_us.model)
        
        for pipeline in [en_us, en_uk]:
            generate_audio(pipeline, "Testing model reuse.")
                
        ms = (time.perf_counter() - start) * 1000
        logger.info(f"‚úì reuse  | {ms:>5.1f}ms for both models")
    except Exception as e:
        logger.error(f"‚úó reuse  | {str(e)}")

if __name__ == '__main__':
    logger.info("Device Selection & Performance")
    logger.info("-" * 40)
    time_synthesis()
    time_synthesis('cuda')
    time_synthesis('cpu') 
    logger.info("-" * 40)
    compare_shared_model()

##### FILE: ./kokoro/examples/make_triton_compatible.py #####

"""
This script makes the ONNX model compatible with Triton inference server.
"""

import sys
import numpy as np
import onnx
import onnxruntime as ort
import onnx_graphsurgeon as gs


def add_squeeze(graph, speed_input, speed_unsqueezed):
    """
    Add squeeze operation to the speed input to change shape from [batch_size, 1] to [batch_size]
    """
    # Create a squeeze node
    squeeze_node = gs.Node(
        op="Squeeze",
        name="speed_squeeze",
        inputs=[speed_unsqueezed],
        outputs=[gs.Variable(name="speed_squeezed", dtype=speed_unsqueezed.dtype)]
    )
    
    ## Find first node that has speed_unsqueezed as input
    insert_idx = 0
    for idx, node in enumerate(graph.nodes):
        for i, input_name in enumerate(node.inputs):
            if input_name.name == speed_unsqueezed.name:
                insert_idx = idx
                break
        if insert_idx != 0:
            break
    
    ## Add squeeze node to the graph
    insert_idx = min(0, insert_idx - 1)
    graph.nodes.insert(insert_idx, squeeze_node)
    
    # Update the speed input to point to the squeezed output
    for node in graph.nodes:
        for i, input_name in enumerate(node.inputs):
            if input_name.name == speed_input.name and not node.name == "speed_squeeze":
                node.inputs[i] = squeeze_node.outputs[0]
                
    return graph


def main():
    if len(sys.argv) != 2:
        print("Usage: python make_triton_compatible.py <onnx_model_path>")
        sys.exit(1)

    onnx_model_path = sys.argv[1]
    onnx_model = onnx.load(onnx_model_path)
    onnx.checker.check_model(onnx_model)
    print("Model is valid")
    
    graph = gs.import_onnx(onnx_model)

    ## get input_id for speed
    speed_idx, speed = None, None
    for idx, input_ in enumerate(graph.inputs):
        if input_.name=="speed":
            speed_idx = idx
            speed = input_

    # Update the speed input to have shape [batch_size, 1]
    speed_unsqueezed = gs.Variable(name="speed", dtype=speed.dtype, shape=[speed.shape[0], 1])
    graph.inputs[speed_idx] = speed_unsqueezed
        
    ## Add squeeze to change speed shape from [batch_size, 1] to [batch_size]
    if speed is not None:
        print(f"Found speed input: {speed.name}")
        print(f"Found speed input shape: {speed.shape}")
        print(f"Found speed input dtype: {speed.dtype}")
        print(f"Found speed input: {speed}")
        print(f"Found speed input: {type(speed)}")
        graph = add_squeeze(graph, speed, speed_unsqueezed)
        
        # Export the modified graph back to ONNX
        modified_model = gs.export_onnx(graph)
        onnx.checker.check_model(modified_model)
        
        # Save the modified model
        output_path = onnx_model_path.replace('.onnx', '_triton.onnx')
        onnx.save(modified_model, output_path)
        print(f"Modified model saved to: {output_path}")
    else:
        print("Speed input not found in the model")


if __name__ == "__main__":
    main()


##### FILE: ./kokoro/examples/export.py #####

import argparse
import os
import torch
import onnx
import onnxruntime as ort
import sounddevice as sd

from kokoro import KModel, KPipeline
from kokoro.model import KModelForONNX

def export_onnx(model, output):
    onnx_file = output + "/" + "kokoro.onnx"

    input_ids = torch.randint(1, 100, (48,)).numpy()
    input_ids = torch.LongTensor([[0, *input_ids, 0]])
    style = torch.randn(1, 256)
    speed = torch.randint(1, 10, (1,)).int()

    torch.onnx.export(
        model, 
        args = (input_ids, style, speed), 
        f = onnx_file, 
        export_params = True, 
        verbose = True, 
        input_names = [ 'input_ids', 'style', 'speed' ], 
        output_names = [ 'waveform', 'duration' ],
        opset_version = 17, 
        dynamic_axes = {
            'input_ids': {0: "batch_size", 1: 'input_ids_len' }, 
            'style': {0: "batch_size"}, 
            "speed": {0: "batch_size"}
        }, 
        do_constant_folding = True, 
    )

    print('export kokoro.onnx ok!')

    onnx_model = onnx.load(onnx_file)
    onnx.checker.check_model(onnx_model)
    print('onnx check ok!')

def load_input_ids(pipeline, text):
    if pipeline.lang_code in 'ab':
        _, tokens = pipeline.g2p(text)
        for gs, ps, tks in pipeline.en_tokenize(tokens):
            if not ps:
                continue
    else:
        ps, _ = pipeline.g2p(text)

    if len(ps) > 510:
        ps = ps[:510]

    input_ids = list(filter(lambda i: i is not None, map(lambda p: pipeline.model.vocab.get(p), ps)))
    print(f"text: {text} -> phonemes: {ps} -> input_ids: {input_ids}")
    input_ids = torch.LongTensor([[0, *input_ids, 0]]).to(pipeline.model.device)
    return ps, input_ids

def load_voice(pipeline, voice, phonemes):
    pack = pipeline.load_voice(voice).to('cpu')
    return pack[len(phonemes) - 1]

def load_sample(model):
    pipeline = KPipeline(lang_code='a', model=model.kmodel, device='cpu')
    text = '''
    In today's fast-paced tech world, building software applications has never been easier ‚Äî thanks to AI-powered coding assistants.'
    '''
    text = '''
    The sky above the port was the color of television, tuned to a dead channel.
    '''
    voice = 'checkpoints/voices/af_heart.pt'

    pipeline = KPipeline(lang_code='z', model=model.kmodel, device='cpu')
    text = '''
    2Êúà15Êó•ÊôöÔºåÁå´Áúº‰∏ì‰∏öÁâàÊï∞ÊçÆÊòæÁ§∫ÔºåÊà™Ëá≥ÂèëÁ®øÔºå„ÄäÂì™Âêí‰πãÈ≠îÁ´•ÈóπÊµ∑„ÄãÔºàÊàñÁß∞„ÄäÂì™Âêí2„ÄãÔºâ‰ªäÊó•Á•®ÊàøÂ∑≤Ëææ7.8‰∫øÂÖÉÔºåÁ¥ØËÆ°Á•®ÊàøÔºàÂê´È¢ÑÂîÆÔºâË∂ÖËøá114‰∫øÂÖÉ„ÄÇ
    '''
    voice = 'checkpoints/voices/zf_xiaoxiao.pt'

    phonemes, input_ids = load_input_ids(pipeline, text)
    style = load_voice(pipeline, voice, phonemes)
    speed = torch.IntTensor([1])

    return input_ids, style, speed

def inference_onnx(model, output):
    onnx_file = output + "/" + "kokoro.onnx"
    session = ort.InferenceSession(onnx_file)

    input_ids, style, speed = load_sample(model)

    outputs = session.run(None, {
        'input_ids': input_ids.numpy(), 
        'style': style.numpy(), 
        'speed': speed.numpy(), 
    })

    output = torch.from_numpy(outputs[0])
    print(f'output: {output.shape}')
    print(output)

    audio = output.numpy()
    sd.play(audio, 24000)
    sd.wait()

def check_model(model):
    input_ids, style, speed = load_sample(model)
    output, duration = model(input_ids, style, speed)

    print(f'output: {output.shape}')
    print(f'duration: {duration.shape}')
    print(output)

    audio = output.numpy()
    sd.play(audio, 24000)
    sd.wait()

if __name__ == "__main__":
    parser = argparse.ArgumentParser("Export kokoro Model to ONNX", add_help=True)
    parser.add_argument("--inference", "-t", help="test kokoro.onnx model", action="store_true")
    parser.add_argument("--check", "-m", help="check kokoro model", action="store_true")
    parser.add_argument(
        "--config_file", "-c", type=str, default="checkpoints/config.json", help="path to config file"
    )
    parser.add_argument(
        "--checkpoint_path", "-p", type=str, default="checkpoints/kokoro-v1_0.pth", help="path to checkpoint file"
    )
    parser.add_argument(
        "--output_dir", "-o", type=str, default="onnx", help="output directory"
    )

    args = parser.parse_args()

    # cfg
    config_file = args.config_file  # change the path of the model config file
    checkpoint_path = args.checkpoint_path  # change the path of the model
    output_dir = args.output_dir
    
    # make dir
    os.makedirs(output_dir, exist_ok=True)

    kmodel = KModel(config=config_file, model=checkpoint_path, disable_complex=True)
    model = KModelForONNX(kmodel).eval()

    if args.inference:
        inference_onnx(model, output_dir)
    elif args.check:
        check_model(model)
    else:
        export_onnx(model, output_dir)


##### FILE: ./kokoro/kokoro/istftnet.py #####

# ADAPTED from https://github.com/yl4579/StyleTTS2/blob/main/Modules/istftnet.py
from kokoro.custom_stft import CustomSTFT
from torch.nn.utils.parametrizations import weight_norm
import math
import torch
import torch.nn as nn
import torch.nn.functional as F


# https://github.com/yl4579/StyleTTS2/blob/main/Modules/utils.py
def init_weights(m, mean=0.0, std=0.01):
    classname = m.__class__.__name__
    if classname.find("Conv") != -1:
        m.weight.data.normal_(mean, std)

def get_padding(kernel_size, dilation=1):
    return int((kernel_size*dilation - dilation)/2)


class AdaIN1d(nn.Module):
    def __init__(self, style_dim, num_features):
        super().__init__()
        # affine should be False, however there's a bug in the old torch.onnx.export (not newer dynamo) that causes the channel dimension to be lost if affine=False. When affine is true, there's additional learnably parameters. This shouldn't really matter setting it to True, since we're in inference mode
        self.norm = nn.InstanceNorm1d(num_features, affine=True)
        self.fc = nn.Linear(style_dim, num_features*2)

    def forward(self, x, s):
        h = self.fc(s)
        h = h.view(h.size(0), h.size(1), 1)
        gamma, beta = torch.chunk(h, chunks=2, dim=1)
        return (1 + gamma) * self.norm(x) + beta


class AdaINResBlock1(nn.Module):
    def __init__(self, channels, kernel_size=3, dilation=(1, 3, 5), style_dim=64):
        super(AdaINResBlock1, self).__init__()
        self.convs1 = nn.ModuleList([
            weight_norm(nn.Conv1d(channels, channels, kernel_size, 1, dilation=dilation[0],
                                  padding=get_padding(kernel_size, dilation[0]))),
            weight_norm(nn.Conv1d(channels, channels, kernel_size, 1, dilation=dilation[1],
                                  padding=get_padding(kernel_size, dilation[1]))),
            weight_norm(nn.Conv1d(channels, channels, kernel_size, 1, dilation=dilation[2],
                                  padding=get_padding(kernel_size, dilation[2])))
        ])
        self.convs1.apply(init_weights)
        self.convs2 = nn.ModuleList([
            weight_norm(nn.Conv1d(channels, channels, kernel_size, 1, dilation=1,
                                  padding=get_padding(kernel_size, 1))),
            weight_norm(nn.Conv1d(channels, channels, kernel_size, 1, dilation=1,
                                  padding=get_padding(kernel_size, 1))),
            weight_norm(nn.Conv1d(channels, channels, kernel_size, 1, dilation=1,
                                  padding=get_padding(kernel_size, 1)))
        ])
        self.convs2.apply(init_weights)
        self.adain1 = nn.ModuleList([
            AdaIN1d(style_dim, channels),
            AdaIN1d(style_dim, channels),
            AdaIN1d(style_dim, channels),
        ])
        self.adain2 = nn.ModuleList([
            AdaIN1d(style_dim, channels),
            AdaIN1d(style_dim, channels),
            AdaIN1d(style_dim, channels),
        ])
        self.alpha1 = nn.ParameterList([nn.Parameter(torch.ones(1, channels, 1)) for i in range(len(self.convs1))])
        self.alpha2 = nn.ParameterList([nn.Parameter(torch.ones(1, channels, 1)) for i in range(len(self.convs2))])

    def forward(self, x, s):
        for c1, c2, n1, n2, a1, a2 in zip(self.convs1, self.convs2, self.adain1, self.adain2, self.alpha1, self.alpha2):
            xt = n1(x, s)
            xt = xt + (1 / a1) * (torch.sin(a1 * xt) ** 2)  # Snake1D
            xt = c1(xt)
            xt = n2(xt, s)
            xt = xt + (1 / a2) * (torch.sin(a2 * xt) ** 2)  # Snake1D
            xt = c2(xt)
            x = xt + x
        return x


class TorchSTFT(nn.Module):
    def __init__(self, filter_length=800, hop_length=200, win_length=800, window='hann'):
        super().__init__()
        self.filter_length = filter_length
        self.hop_length = hop_length
        self.win_length = win_length
        assert window == 'hann', window
        self.window = torch.hann_window(win_length, periodic=True, dtype=torch.float32)

    def transform(self, input_data):
        forward_transform = torch.stft(
            input_data,
            self.filter_length, self.hop_length, self.win_length, window=self.window.to(input_data.device),
            return_complex=True)
        return torch.abs(forward_transform), torch.angle(forward_transform)

    def inverse(self, magnitude, phase):
        inverse_transform = torch.istft(
            magnitude * torch.exp(phase * 1j),
            self.filter_length, self.hop_length, self.win_length, window=self.window.to(magnitude.device))
        return inverse_transform.unsqueeze(-2)  # unsqueeze to stay consistent with conv_transpose1d implementation

    def forward(self, input_data):
        self.magnitude, self.phase = self.transform(input_data)
        reconstruction = self.inverse(self.magnitude, self.phase)
        return reconstruction


class SineGen(nn.Module):
    """ Definition of sine generator
    SineGen(samp_rate, harmonic_num = 0,
            sine_amp = 0.1, noise_std = 0.003,
            voiced_threshold = 0,
            flag_for_pulse=False)
    samp_rate: sampling rate in Hz
    harmonic_num: number of harmonic overtones (default 0)
    sine_amp: amplitude of sine-wavefrom (default 0.1)
    noise_std: std of Gaussian noise (default 0.003)
    voiced_thoreshold: F0 threshold for U/V classification (default 0)
    flag_for_pulse: this SinGen is used inside PulseGen (default False)
    Note: when flag_for_pulse is True, the first time step of a voiced
        segment is always sin(torch.pi) or cos(0)
    """
    def __init__(self, samp_rate, upsample_scale, harmonic_num=0,
                 sine_amp=0.1, noise_std=0.003,
                 voiced_threshold=0,
                 flag_for_pulse=False):
        super(SineGen, self).__init__()
        self.sine_amp = sine_amp
        self.noise_std = noise_std
        self.harmonic_num = harmonic_num
        self.dim = self.harmonic_num + 1
        self.sampling_rate = samp_rate
        self.voiced_threshold = voiced_threshold
        self.flag_for_pulse = flag_for_pulse
        self.upsample_scale = upsample_scale

    def _f02uv(self, f0):
        # generate uv signal
        uv = (f0 > self.voiced_threshold).type(torch.float32)
        return uv

    def _f02sine(self, f0_values):
        """ f0_values: (batchsize, length, dim)
            where dim indicates fundamental tone and overtones
        """
        # convert to F0 in rad. The interger part n can be ignored
        # because 2 * torch.pi * n doesn't affect phase
        rad_values = (f0_values / self.sampling_rate) % 1
        # initial phase noise (no noise for fundamental component)
        rand_ini = torch.rand(f0_values.shape[0], f0_values.shape[2], device=f0_values.device)
        rand_ini[:, 0] = 0
        rad_values[:, 0, :] = rad_values[:, 0, :] + rand_ini
        # instantanouse phase sine[t] = sin(2*pi \sum_i=1 ^{t} rad)
        if not self.flag_for_pulse:
            rad_values = F.interpolate(rad_values.transpose(1, 2), scale_factor=1/self.upsample_scale, mode="linear").transpose(1, 2)
            phase = torch.cumsum(rad_values, dim=1) * 2 * torch.pi
            phase = F.interpolate(phase.transpose(1, 2) * self.upsample_scale, scale_factor=self.upsample_scale, mode="linear").transpose(1, 2)
            sines = torch.sin(phase)
        else:
            # If necessary, make sure that the first time step of every
            # voiced segments is sin(pi) or cos(0)
            # This is used for pulse-train generation
            # identify the last time step in unvoiced segments
            uv = self._f02uv(f0_values)
            uv_1 = torch.roll(uv, shifts=-1, dims=1)
            uv_1[:, -1, :] = 1
            u_loc = (uv < 1) * (uv_1 > 0)
            # get the instantanouse phase
            tmp_cumsum = torch.cumsum(rad_values, dim=1)
            # different batch needs to be processed differently
            for idx in range(f0_values.shape[0]):
                temp_sum = tmp_cumsum[idx, u_loc[idx, :, 0], :]
                temp_sum[1:, :] = temp_sum[1:, :] - temp_sum[0:-1, :]
                # stores the accumulation of i.phase within
                # each voiced segments
                tmp_cumsum[idx, :, :] = 0
                tmp_cumsum[idx, u_loc[idx, :, 0], :] = temp_sum
            # rad_values - tmp_cumsum: remove the accumulation of i.phase
            # within the previous voiced segment.
            i_phase = torch.cumsum(rad_values - tmp_cumsum, dim=1)
            # get the sines
            sines = torch.cos(i_phase * 2 * torch.pi)
        return sines

    def forward(self, f0):
        """ sine_tensor, uv = forward(f0)
        input F0: tensor(batchsize=1, length, dim=1)
                  f0 for unvoiced steps should be 0
        output sine_tensor: tensor(batchsize=1, length, dim)
        output uv: tensor(batchsize=1, length, 1)
        """
        f0_buf = torch.zeros(f0.shape[0], f0.shape[1], self.dim, device=f0.device)
        # fundamental component
        fn = torch.multiply(f0, torch.FloatTensor([[range(1, self.harmonic_num + 2)]]).to(f0.device))
        # generate sine waveforms
        sine_waves = self._f02sine(fn) * self.sine_amp
        # generate uv signal
        # uv = torch.ones(f0.shape)
        # uv = uv * (f0 > self.voiced_threshold)
        uv = self._f02uv(f0)
        # noise: for unvoiced should be similar to sine_amp
        #        std = self.sine_amp/3 -> max value ~ self.sine_amp
        #        for voiced regions is self.noise_std
        noise_amp = uv * self.noise_std + (1 - uv) * self.sine_amp / 3
        noise = noise_amp * torch.randn_like(sine_waves)
        # first: set the unvoiced part to 0 by uv
        # then: additive noise
        sine_waves = sine_waves * uv + noise
        return sine_waves, uv, noise


class SourceModuleHnNSF(nn.Module):
    """ SourceModule for hn-nsf
    SourceModule(sampling_rate, harmonic_num=0, sine_amp=0.1,
                 add_noise_std=0.003, voiced_threshod=0)
    sampling_rate: sampling_rate in Hz
    harmonic_num: number of harmonic above F0 (default: 0)
    sine_amp: amplitude of sine source signal (default: 0.1)
    add_noise_std: std of additive Gaussian noise (default: 0.003)
        note that amplitude of noise in unvoiced is decided
        by sine_amp
    voiced_threshold: threhold to set U/V given F0 (default: 0)
    Sine_source, noise_source = SourceModuleHnNSF(F0_sampled)
    F0_sampled (batchsize, length, 1)
    Sine_source (batchsize, length, 1)
    noise_source (batchsize, length 1)
    uv (batchsize, length, 1)
    """
    def __init__(self, sampling_rate, upsample_scale, harmonic_num=0, sine_amp=0.1,
                 add_noise_std=0.003, voiced_threshod=0):
        super(SourceModuleHnNSF, self).__init__()
        self.sine_amp = sine_amp
        self.noise_std = add_noise_std
        # to produce sine waveforms
        self.l_sin_gen = SineGen(sampling_rate, upsample_scale, harmonic_num,
                                 sine_amp, add_noise_std, voiced_threshod)
        # to merge source harmonics into a single excitation
        self.l_linear = nn.Linear(harmonic_num + 1, 1)
        self.l_tanh = nn.Tanh()

    def forward(self, x):
        """
        Sine_source, noise_source = SourceModuleHnNSF(F0_sampled)
        F0_sampled (batchsize, length, 1)
        Sine_source (batchsize, length, 1)
        noise_source (batchsize, length 1)
        """
        # source for harmonic branch
        with torch.no_grad():
            sine_wavs, uv, _ = self.l_sin_gen(x)
        sine_merge = self.l_tanh(self.l_linear(sine_wavs))
        # source for noise branch, in the same shape as uv
        noise = torch.randn_like(uv) * self.sine_amp / 3
        return sine_merge, noise, uv


class Generator(nn.Module):
    def __init__(self, style_dim, resblock_kernel_sizes, upsample_rates, upsample_initial_channel, resblock_dilation_sizes, upsample_kernel_sizes, gen_istft_n_fft, gen_istft_hop_size, disable_complex=False):
        super(Generator, self).__init__()
        self.num_kernels = len(resblock_kernel_sizes)
        self.num_upsamples = len(upsample_rates)
        self.m_source = SourceModuleHnNSF(
                    sampling_rate=24000,
                    upsample_scale=math.prod(upsample_rates) * gen_istft_hop_size,
                    harmonic_num=8, voiced_threshod=10)
        self.f0_upsamp = nn.Upsample(scale_factor=math.prod(upsample_rates) * gen_istft_hop_size)
        self.noise_convs = nn.ModuleList()
        self.noise_res = nn.ModuleList()
        self.ups = nn.ModuleList()
        for i, (u, k) in enumerate(zip(upsample_rates, upsample_kernel_sizes)):
            self.ups.append(weight_norm(
                nn.ConvTranspose1d(upsample_initial_channel//(2**i), upsample_initial_channel//(2**(i+1)),
                                   k, u, padding=(k-u)//2)))
        self.resblocks = nn.ModuleList()
        for i in range(len(self.ups)):
            ch = upsample_initial_channel//(2**(i+1))
            for j, (k, d) in enumerate(zip(resblock_kernel_sizes,resblock_dilation_sizes)):
                self.resblocks.append(AdaINResBlock1(ch, k, d, style_dim))
            c_cur = upsample_initial_channel // (2 ** (i + 1))
            if i + 1 < len(upsample_rates):
                stride_f0 = math.prod(upsample_rates[i + 1:])
                self.noise_convs.append(nn.Conv1d(
                    gen_istft_n_fft + 2, c_cur, kernel_size=stride_f0 * 2, stride=stride_f0, padding=(stride_f0+1) // 2))
                self.noise_res.append(AdaINResBlock1(c_cur, 7, [1,3,5], style_dim))
            else:
                self.noise_convs.append(nn.Conv1d(gen_istft_n_fft + 2, c_cur, kernel_size=1))
                self.noise_res.append(AdaINResBlock1(c_cur, 11, [1,3,5], style_dim))
        self.post_n_fft = gen_istft_n_fft
        self.conv_post = weight_norm(nn.Conv1d(ch, self.post_n_fft + 2, 7, 1, padding=3))
        self.ups.apply(init_weights)
        self.conv_post.apply(init_weights)
        self.reflection_pad = nn.ReflectionPad1d((1, 0))
        self.stft = (
            CustomSTFT(filter_length=gen_istft_n_fft, hop_length=gen_istft_hop_size, win_length=gen_istft_n_fft)
            if disable_complex
            else TorchSTFT(filter_length=gen_istft_n_fft, hop_length=gen_istft_hop_size, win_length=gen_istft_n_fft)
        )

    def forward(self, x, s, f0):
        with torch.no_grad():
            f0 = self.f0_upsamp(f0[:, None]).transpose(1, 2)  # bs,n,t
            har_source, noi_source, uv = self.m_source(f0)
            har_source = har_source.transpose(1, 2).squeeze(1)
            har_spec, har_phase = self.stft.transform(har_source)
            har = torch.cat([har_spec, har_phase], dim=1)
        for i in range(self.num_upsamples):
            x = F.leaky_relu(x, negative_slope=0.1) 
            x_source = self.noise_convs[i](har)
            x_source = self.noise_res[i](x_source, s)
            x = self.ups[i](x)
            if i == self.num_upsamples - 1:
                x = self.reflection_pad(x)
            x = x + x_source
            xs = None
            for j in range(self.num_kernels):
                if xs is None:
                    xs = self.resblocks[i*self.num_kernels+j](x, s)
                else:
                    xs += self.resblocks[i*self.num_kernels+j](x, s)
            x = xs / self.num_kernels
        x = F.leaky_relu(x)
        x = self.conv_post(x)
        spec = torch.exp(x[:,:self.post_n_fft // 2 + 1, :])
        phase = torch.sin(x[:, self.post_n_fft // 2 + 1:, :])
        return self.stft.inverse(spec, phase)


class UpSample1d(nn.Module):
    def __init__(self, layer_type):
        super().__init__()
        self.layer_type = layer_type

    def forward(self, x):
        if self.layer_type == 'none':
            return x
        else:
            return F.interpolate(x, scale_factor=2, mode='nearest')


class AdainResBlk1d(nn.Module):
    def __init__(self, dim_in, dim_out, style_dim=64, actv=nn.LeakyReLU(0.2), upsample='none', dropout_p=0.0):
        super().__init__()
        self.actv = actv
        self.upsample_type = upsample
        self.upsample = UpSample1d(upsample)
        self.learned_sc = dim_in != dim_out
        self._build_weights(dim_in, dim_out, style_dim)
        self.dropout = nn.Dropout(dropout_p)
        if upsample == 'none':
            self.pool = nn.Identity()
        else:
            self.pool = weight_norm(nn.ConvTranspose1d(dim_in, dim_in, kernel_size=3, stride=2, groups=dim_in, padding=1, output_padding=1))

    def _build_weights(self, dim_in, dim_out, style_dim):
        self.conv1 = weight_norm(nn.Conv1d(dim_in, dim_out, 3, 1, 1))
        self.conv2 = weight_norm(nn.Conv1d(dim_out, dim_out, 3, 1, 1))
        self.norm1 = AdaIN1d(style_dim, dim_in)
        self.norm2 = AdaIN1d(style_dim, dim_out)
        if self.learned_sc:
            self.conv1x1 = weight_norm(nn.Conv1d(dim_in, dim_out, 1, 1, 0, bias=False))

    def _shortcut(self, x):
        x = self.upsample(x)
        if self.learned_sc:
            x = self.conv1x1(x)
        return x

    def _residual(self, x, s):
        x = self.norm1(x, s)
        x = self.actv(x)
        x = self.pool(x)
        x = self.conv1(self.dropout(x))
        x = self.norm2(x, s)
        x = self.actv(x)
        x = self.conv2(self.dropout(x))
        return x

    def forward(self, x, s):
        out = self._residual(x, s)
        out = (out + self._shortcut(x)) * torch.rsqrt(torch.tensor(2))
        return out


class Decoder(nn.Module):
    def __init__(self, dim_in, style_dim, dim_out, 
                 resblock_kernel_sizes,
                 upsample_rates,
                 upsample_initial_channel,
                 resblock_dilation_sizes,
                 upsample_kernel_sizes,
                 gen_istft_n_fft, gen_istft_hop_size,
                 disable_complex=False):
        super().__init__()
        self.encode = AdainResBlk1d(dim_in + 2, 1024, style_dim)
        self.decode = nn.ModuleList()
        self.decode.append(AdainResBlk1d(1024 + 2 + 64, 1024, style_dim))
        self.decode.append(AdainResBlk1d(1024 + 2 + 64, 1024, style_dim))
        self.decode.append(AdainResBlk1d(1024 + 2 + 64, 1024, style_dim))
        self.decode.append(AdainResBlk1d(1024 + 2 + 64, 512, style_dim, upsample=True))
        self.F0_conv = weight_norm(nn.Conv1d(1, 1, kernel_size=3, stride=2, groups=1, padding=1))
        self.N_conv = weight_norm(nn.Conv1d(1, 1, kernel_size=3, stride=2, groups=1, padding=1))
        self.asr_res = nn.Sequential(weight_norm(nn.Conv1d(512, 64, kernel_size=1)))
        self.generator = Generator(style_dim, resblock_kernel_sizes, upsample_rates, 
                                   upsample_initial_channel, resblock_dilation_sizes, 
                                   upsample_kernel_sizes, gen_istft_n_fft, gen_istft_hop_size, disable_complex=disable_complex)

    def forward(self, asr, F0_curve, N, s):
        F0 = self.F0_conv(F0_curve.unsqueeze(1))
        N = self.N_conv(N.unsqueeze(1))
        x = torch.cat([asr, F0, N], axis=1)
        x = self.encode(x, s)
        asr_res = self.asr_res(asr)
        res = True
        for block in self.decode:
            if res:
                x = torch.cat([x, asr_res, F0, N], axis=1)
            x = block(x, s)
            if block.upsample_type != "none":
                res = False
        x = self.generator(x, s, F0_curve)
        return x


##### FILE: ./kokoro/kokoro/__init__.py #####

__version__ = '0.9.4'

from loguru import logger
import sys

# Remove default handler
logger.remove()

# Add custom handler with clean format including module and line number
logger.add(
    sys.stderr,
    format="<green>{time:HH:mm:ss}</green> | <cyan>{module:>16}:{line}</cyan> | <level>{level: >8}</level> | <level>{message}</level>",
    colorize=True,
    level="INFO" # "DEBUG" to enable logger.debug("message") and up prints 
                 # "ERROR" to enable only logger.error("message") prints
                 # etc
)

# Disable before release or as needed
logger.disable("kokoro")

from .model import KModel
from .pipeline import KPipeline


##### FILE: ./kokoro/kokoro/model.py #####

from .istftnet import Decoder
from .modules import CustomAlbert, ProsodyPredictor, TextEncoder
from dataclasses import dataclass
from huggingface_hub import hf_hub_download
from loguru import logger
from transformers import AlbertConfig
from typing import Dict, Optional, Union
import json
import torch

class KModel(torch.nn.Module):
    '''
    KModel is a torch.nn.Module with 2 main responsibilities:
    1. Init weights, downloading config.json + model.pth from HF if needed
    2. forward(phonemes: str, ref_s: FloatTensor) -> (audio: FloatTensor)

    You likely only need one KModel instance, and it can be reused across
    multiple KPipelines to avoid redundant memory allocation.

    Unlike KPipeline, KModel is language-blind.

    KModel stores self.vocab and thus knows how to map phonemes -> input_ids,
    so there is no need to repeatedly download config.json outside of KModel.
    '''

    MODEL_NAMES = {
        'hexgrad/Kokoro-82M': 'kokoro-v1_0.pth',
        'hexgrad/Kokoro-82M-v1.1-zh': 'kokoro-v1_1-zh.pth',
    }

    def __init__(
        self,
        repo_id: Optional[str] = None,
        config: Union[Dict, str, None] = None,
        model: Optional[str] = None,
        disable_complex: bool = False
    ):
        super().__init__()
        if repo_id is None:
            repo_id = 'hexgrad/Kokoro-82M'
            print(f"WARNING: Defaulting repo_id to {repo_id}. Pass repo_id='{repo_id}' to suppress this warning.")
        self.repo_id = repo_id
        if not isinstance(config, dict):
            if not config:
                logger.debug("No config provided, downloading from HF")
                config = hf_hub_download(repo_id=repo_id, filename='config.json')
            with open(config, 'r', encoding='utf-8') as r:
                config = json.load(r)
                logger.debug(f"Loaded config: {config}")
        self.vocab = config['vocab']
        self.bert = CustomAlbert(AlbertConfig(vocab_size=config['n_token'], **config['plbert']))
        self.bert_encoder = torch.nn.Linear(self.bert.config.hidden_size, config['hidden_dim'])
        self.context_length = self.bert.config.max_position_embeddings
        self.predictor = ProsodyPredictor(
            style_dim=config['style_dim'], d_hid=config['hidden_dim'],
            nlayers=config['n_layer'], max_dur=config['max_dur'], dropout=config['dropout']
        )
        self.text_encoder = TextEncoder(
            channels=config['hidden_dim'], kernel_size=config['text_encoder_kernel_size'],
            depth=config['n_layer'], n_symbols=config['n_token']
        )
        self.decoder = Decoder(
            dim_in=config['hidden_dim'], style_dim=config['style_dim'],
            dim_out=config['n_mels'], disable_complex=disable_complex, **config['istftnet']
        )
        if not model:
            model = hf_hub_download(repo_id=repo_id, filename=KModel.MODEL_NAMES[repo_id])
        for key, state_dict in torch.load(model, map_location='cpu', weights_only=True).items():
            assert hasattr(self, key), key
            try:
                getattr(self, key).load_state_dict(state_dict)
            except:
                logger.debug(f"Did not load {key} from state_dict")
                state_dict = {k[7:]: v for k, v in state_dict.items()}
                getattr(self, key).load_state_dict(state_dict, strict=False)

    @property
    def device(self):
        return self.bert.device

    @dataclass
    class Output:
        audio: torch.FloatTensor
        pred_dur: Optional[torch.LongTensor] = None

    @torch.no_grad()
    def forward_with_tokens(
        self,
        input_ids: torch.LongTensor,
        ref_s: torch.FloatTensor,
        speed: float = 1
    ) -> tuple[torch.FloatTensor, torch.LongTensor]:
        input_lengths = torch.full(
            (input_ids.shape[0],), 
            input_ids.shape[-1], 
            device=input_ids.device,
            dtype=torch.long
        )

        text_mask = torch.arange(input_lengths.max()).unsqueeze(0).expand(input_lengths.shape[0], -1).type_as(input_lengths)
        text_mask = torch.gt(text_mask+1, input_lengths.unsqueeze(1)).to(self.device)
        bert_dur = self.bert(input_ids, attention_mask=(~text_mask).int())
        d_en = self.bert_encoder(bert_dur).transpose(-1, -2)
        s = ref_s[:, 128:]
        d = self.predictor.text_encoder(d_en, s, input_lengths, text_mask)
        x, _ = self.predictor.lstm(d)
        duration = self.predictor.duration_proj(x)
        duration = torch.sigmoid(duration).sum(axis=-1) / speed
        pred_dur = torch.round(duration).clamp(min=1).long().squeeze()
        indices = torch.repeat_interleave(torch.arange(input_ids.shape[1], device=self.device), pred_dur)
        pred_aln_trg = torch.zeros((input_ids.shape[1], indices.shape[0]), device=self.device)
        pred_aln_trg[indices, torch.arange(indices.shape[0])] = 1
        pred_aln_trg = pred_aln_trg.unsqueeze(0).to(self.device)
        en = d.transpose(-1, -2) @ pred_aln_trg
        F0_pred, N_pred = self.predictor.F0Ntrain(en, s)
        t_en = self.text_encoder(input_ids, input_lengths, text_mask)
        asr = t_en @ pred_aln_trg
        audio = self.decoder(asr, F0_pred, N_pred, ref_s[:, :128]).squeeze()
        return audio, pred_dur

    def forward(
        self,
        phonemes: str,
        ref_s: torch.FloatTensor,
        speed: float = 1,
        return_output: bool = False
    ) -> Union['KModel.Output', torch.FloatTensor]:
        input_ids = list(filter(lambda i: i is not None, map(lambda p: self.vocab.get(p), phonemes)))
        logger.debug(f"phonemes: {phonemes} -> input_ids: {input_ids}")
        assert len(input_ids)+2 <= self.context_length, (len(input_ids)+2, self.context_length)
        input_ids = torch.LongTensor([[0, *input_ids, 0]]).to(self.device)
        ref_s = ref_s.to(self.device)
        audio, pred_dur = self.forward_with_tokens(input_ids, ref_s, speed)
        audio = audio.squeeze().cpu()
        pred_dur = pred_dur.cpu() if pred_dur is not None else None
        logger.debug(f"pred_dur: {pred_dur}")
        return self.Output(audio=audio, pred_dur=pred_dur) if return_output else audio

class KModelForONNX(torch.nn.Module):
    def __init__(self, kmodel: KModel):
        super().__init__()
        self.kmodel = kmodel

    def forward(
        self,
        input_ids: torch.LongTensor,
        ref_s: torch.FloatTensor,
        speed: float = 1
    ) -> tuple[torch.FloatTensor, torch.LongTensor]:
        waveform, duration = self.kmodel.forward_with_tokens(input_ids, ref_s, speed)
        return waveform, duration


##### FILE: ./kokoro/kokoro/custom_stft.py #####

from attr import attr
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

class CustomSTFT(nn.Module):
    """
    STFT/iSTFT without unfold/complex ops, using conv1d and conv_transpose1d.

    - forward STFT => Real-part conv1d + Imag-part conv1d
    - inverse STFT => Real-part conv_transpose1d + Imag-part conv_transpose1d + sum
    - avoids F.unfold, so easier to export to ONNX
    - uses replicate or constant padding for 'center=True' to approximate 'reflect' 
      (reflect is not supported for dynamic shapes in ONNX)
    """

    def __init__(
        self,
        filter_length=800,
        hop_length=200,
        win_length=800,
        window="hann",
        center=True,
        pad_mode="replicate",  # or 'constant'
    ):
        super().__init__()
        self.filter_length = filter_length
        self.hop_length = hop_length
        self.win_length = win_length
        self.n_fft = filter_length
        self.center = center
        self.pad_mode = pad_mode

        # Number of frequency bins for real-valued STFT with onesided=True
        self.freq_bins = self.n_fft // 2 + 1

        # Build window
        assert window == 'hann', window
        window_tensor = torch.hann_window(win_length, periodic=True, dtype=torch.float32)
        if self.win_length < self.n_fft:
            # Zero-pad up to n_fft
            extra = self.n_fft - self.win_length
            window_tensor = F.pad(window_tensor, (0, extra))
        elif self.win_length > self.n_fft:
            window_tensor = window_tensor[: self.n_fft]
        self.register_buffer("window", window_tensor)

        # Precompute forward DFT (real, imag)
        # PyTorch stft uses e^{-j 2 pi k n / N} => real=cos(...), imag=-sin(...)
        n = np.arange(self.n_fft)
        k = np.arange(self.freq_bins)
        angle = 2 * np.pi * np.outer(k, n) / self.n_fft  # shape (freq_bins, n_fft)
        dft_real = np.cos(angle)
        dft_imag = -np.sin(angle)  # note negative sign

        # Combine window and dft => shape (freq_bins, filter_length)
        # We'll make 2 conv weight tensors of shape (freq_bins, 1, filter_length).
        forward_window = window_tensor.numpy()  # shape (n_fft,)
        forward_real = dft_real * forward_window  # (freq_bins, n_fft)
        forward_imag = dft_imag * forward_window

        # Convert to PyTorch
        forward_real_torch = torch.from_numpy(forward_real).float()
        forward_imag_torch = torch.from_numpy(forward_imag).float()

        # Register as Conv1d weight => (out_channels, in_channels, kernel_size)
        # out_channels = freq_bins, in_channels=1, kernel_size=n_fft
        self.register_buffer(
            "weight_forward_real", forward_real_torch.unsqueeze(1)
        )
        self.register_buffer(
            "weight_forward_imag", forward_imag_torch.unsqueeze(1)
        )

        # Precompute inverse DFT
        # Real iFFT formula => scale = 1/n_fft, doubling for bins 1..freq_bins-2 if n_fft even, etc.
        # For simplicity, we won't do the "DC/nyquist not doubled" approach here. 
        # If you want perfect real iSTFT, you can add that logic. 
        # This version just yields good approximate reconstruction with Hann + typical overlap.
        inv_scale = 1.0 / self.n_fft
        n = np.arange(self.n_fft)
        angle_t = 2 * np.pi * np.outer(n, k) / self.n_fft  # shape (n_fft, freq_bins)
        idft_cos = np.cos(angle_t).T  # => (freq_bins, n_fft)
        idft_sin = np.sin(angle_t).T  # => (freq_bins, n_fft)

        # Multiply by window again for typical overlap-add
        # We also incorporate the scale factor 1/n_fft
        inv_window = window_tensor.numpy() * inv_scale
        backward_real = idft_cos * inv_window  # (freq_bins, n_fft)
        backward_imag = idft_sin * inv_window

        # We'll implement iSTFT as real+imag conv_transpose with stride=hop.
        self.register_buffer(
            "weight_backward_real", torch.from_numpy(backward_real).float().unsqueeze(1)
        )
        self.register_buffer(
            "weight_backward_imag", torch.from_numpy(backward_imag).float().unsqueeze(1)
        )
        


    def transform(self, waveform: torch.Tensor):
        """
        Forward STFT => returns magnitude, phase
        Output shape => (batch, freq_bins, frames)
        """
        # waveform shape => (B, T).  conv1d expects (B, 1, T).
        # Optional center pad
        if self.center:
            pad_len = self.n_fft // 2
            waveform = F.pad(waveform, (pad_len, pad_len), mode=self.pad_mode)

        x = waveform.unsqueeze(1)  # => (B, 1, T)
        # Convolution to get real part => shape (B, freq_bins, frames)
        real_out = F.conv1d(
            x,
            self.weight_forward_real,
            bias=None,
            stride=self.hop_length,
            padding=0,
        )
        # Imag part
        imag_out = F.conv1d(
            x,
            self.weight_forward_imag,
            bias=None,
            stride=self.hop_length,
            padding=0,
        )

        # magnitude, phase
        magnitude = torch.sqrt(real_out**2 + imag_out**2 + 1e-14)
        phase = torch.atan2(imag_out, real_out)
        # Handle the case where imag_out is 0 and real_out is negative to correct ONNX atan2 to match PyTorch
        # In this case, PyTorch returns pi, ONNX returns -pi
        correction_mask = (imag_out == 0) & (real_out < 0)
        phase[correction_mask] = torch.pi
        return magnitude, phase


    def inverse(self, magnitude: torch.Tensor, phase: torch.Tensor, length=None):
        """
        Inverse STFT => returns waveform shape (B, T).
        """
        # magnitude, phase => (B, freq_bins, frames)
        # Re-create real/imag => shape (B, freq_bins, frames)
        real_part = magnitude * torch.cos(phase)
        imag_part = magnitude * torch.sin(phase)

        # conv_transpose wants shape (B, freq_bins, frames). We'll treat "frames" as time dimension
        # so we do (B, freq_bins, frames) => (B, freq_bins, frames)
        # But PyTorch conv_transpose1d expects (B, in_channels, input_length)
        real_part = real_part  # (B, freq_bins, frames)
        imag_part = imag_part

        # real iSTFT => convolve with "backward_real", "backward_imag", and sum
        # We'll do 2 conv_transpose calls, each giving (B, 1, time),
        # then add them => (B, 1, time).
        real_rec = F.conv_transpose1d(
            real_part,
            self.weight_backward_real,  # shape (freq_bins, 1, filter_length)
            bias=None,
            stride=self.hop_length,
            padding=0,
        )
        imag_rec = F.conv_transpose1d(
            imag_part,
            self.weight_backward_imag,
            bias=None,
            stride=self.hop_length,
            padding=0,
        )
        # sum => (B, 1, time)
        waveform = real_rec - imag_rec  # typical real iFFT has minus for imaginary part

        # If we used "center=True" in forward, we should remove pad
        if self.center:
            pad_len = self.n_fft // 2
            # Because of transposed convolution, total length might have extra samples
            # We remove `pad_len` from start & end if possible
            waveform = waveform[..., pad_len:-pad_len]

        # If a specific length is desired, clamp
        if length is not None:
            waveform = waveform[..., :length]

        # shape => (B, T)
        return waveform

    def forward(self, x: torch.Tensor):
        """
        Full STFT -> iSTFT pass: returns time-domain reconstruction.
        Same interface as your original code.
        """
        mag, phase = self.transform(x)
        return self.inverse(mag, phase, length=x.shape[-1])


##### FILE: ./kokoro/kokoro/pipeline.py #####

from .model import KModel
from dataclasses import dataclass
from huggingface_hub import hf_hub_download
from loguru import logger
from misaki import en, espeak
from typing import Callable, Generator, List, Optional, Tuple, Union
import re
import torch
import os

ALIASES = {
    'en-us': 'a',
    'en-gb': 'b',
    'es': 'e',
    'fr-fr': 'f',
    'hi': 'h',
    'it': 'i',
    'pt-br': 'p',
    'ja': 'j',
    'zh': 'z',
}

LANG_CODES = dict(
    # pip install misaki[en]
    a='American English',
    b='British English',

    # espeak-ng
    e='es',
    f='fr-fr',
    h='hi',
    i='it',
    p='pt-br',

    # pip install misaki[ja]
    j='Japanese',

    # pip install misaki[zh]
    z='Mandarin Chinese',
)

class KPipeline:
    '''
    KPipeline is a language-aware support class with 2 main responsibilities:
    1. Perform language-specific G2P, mapping (and chunking) text -> phonemes
    2. Manage and store voices, lazily downloaded from HF if needed

    You are expected to have one KPipeline per language. If you have multiple
    KPipelines, you should reuse one KModel instance across all of them.

    KPipeline is designed to work with a KModel, but this is not required.
    There are 2 ways to pass an existing model into a pipeline:
    1. On init: us_pipeline = KPipeline(lang_code='a', model=model)
    2. On call: us_pipeline(text, voice, model=model)

    By default, KPipeline will automatically initialize its own KModel. To
    suppress this, construct a "quiet" KPipeline with model=False.

    A "quiet" KPipeline yields (graphemes, phonemes, None) without generating
    any audio. You can use this to phonemize and chunk your text in advance.

    A "loud" KPipeline _with_ a model yields (graphemes, phonemes, audio).
    '''
    def __init__(
        self,
        lang_code: str,
        repo_id: Optional[str] = None,
        model: Union[KModel, bool] = True,
        trf: bool = False,
        en_callable: Optional[Callable[[str], str]] = None,
        device: Optional[str] = None
    ):
        """Initialize a KPipeline.
        
        Args:
            lang_code: Language code for G2P processing
            model: KModel instance, True to create new model, False for no model
            trf: Whether to use transformer-based G2P
            device: Override default device selection ('cuda' or 'cpu', or None for auto)
                   If None, will auto-select cuda if available
                   If 'cuda' and not available, will explicitly raise an error
        """
        if repo_id is None:
            repo_id = 'hexgrad/Kokoro-82M'
            print(f"WARNING: Defaulting repo_id to {repo_id}. Pass repo_id='{repo_id}' to suppress this warning.")
        self.repo_id = repo_id
        lang_code = lang_code.lower()
        lang_code = ALIASES.get(lang_code, lang_code)
        assert lang_code in LANG_CODES, (lang_code, LANG_CODES)
        self.lang_code = lang_code
        self.model = None
        if isinstance(model, KModel):
            self.model = model
        elif model:
            if device == 'cuda' and not torch.cuda.is_available():
                raise RuntimeError("CUDA requested but not available")
            if device == 'mps' and not torch.backends.mps.is_available():
                raise RuntimeError("MPS requested but not available")
            if device == 'mps' and os.environ.get('PYTORCH_ENABLE_MPS_FALLBACK') != '1':
                raise RuntimeError("MPS requested but fallback not enabled")
            if device is None:
                if torch.cuda.is_available():
                    device = 'cuda'
                elif os.environ.get('PYTORCH_ENABLE_MPS_FALLBACK') == '1' and torch.backends.mps.is_available():
                    device = 'mps'
                else:
                    device = 'cpu'
            try:
                self.model = KModel(repo_id=repo_id).to(device).eval()
            except RuntimeError as e:
                if device == 'cuda':
                    raise RuntimeError(f"""Failed to initialize model on CUDA: {e}. 
                                       Try setting device='cpu' or check CUDA installation.""")
                raise
        self.voices = {}
        if lang_code in 'ab':
            try:
                fallback = espeak.EspeakFallback(british=lang_code=='b')
            except Exception as e:
                logger.warning("EspeakFallback not Enabled: OOD words will be skipped")
                logger.warning({str(e)})
                fallback = None
            self.g2p = en.G2P(trf=trf, british=lang_code=='b', fallback=fallback, unk='')
        elif lang_code == 'j':
            try:
                from misaki import ja
                self.g2p = ja.JAG2P()
            except ImportError:
                logger.error("You need to `pip install misaki[ja]` to use lang_code='j'")
                raise
        elif lang_code == 'z':
            try:
                from misaki import zh
                self.g2p = zh.ZHG2P(
                    version=None if repo_id.endswith('/Kokoro-82M') else '1.1',
                    en_callable=en_callable
                )
            except ImportError:
                logger.error("You need to `pip install misaki[zh]` to use lang_code='z'")
                raise
        else:
            language = LANG_CODES[lang_code]
            logger.warning(f"Using EspeakG2P(language='{language}'). Chunking logic not yet implemented, so long texts may be truncated unless you split them with '\\n'.")
            self.g2p = espeak.EspeakG2P(language=language)

    def load_single_voice(self, voice: str):
        if voice in self.voices:
            return self.voices[voice]
        if voice.endswith('.pt'):
            f = voice
        else:
            f = hf_hub_download(repo_id=self.repo_id, filename=f'voices/{voice}.pt')
            if not voice.startswith(self.lang_code):
                v = LANG_CODES.get(voice, voice)
                p = LANG_CODES.get(self.lang_code, self.lang_code)
                logger.warning(f'Language mismatch, loading {v} voice into {p} pipeline.')
        pack = torch.load(f, weights_only=True)
        self.voices[voice] = pack
        return pack

    """
    load_voice is a helper function that lazily downloads and loads a voice:
    Single voice can be requested (e.g. 'af_bella') or multiple voices (e.g. 'af_bella,af_jessica').
    If multiple voices are requested, they are averaged.
    Delimiter is optional and defaults to ','.
    """
    def load_voice(self, voice: Union[str, torch.FloatTensor], delimiter: str = ",") -> torch.FloatTensor:
        if isinstance(voice, torch.FloatTensor):
            return voice
        if voice in self.voices:
            return self.voices[voice]
        logger.debug(f"Loading voice: {voice}")
        packs = [self.load_single_voice(v) for v in voice.split(delimiter)]
        if len(packs) == 1:
            return packs[0]
        self.voices[voice] = torch.mean(torch.stack(packs), dim=0)
        return self.voices[voice]

    @staticmethod
    def tokens_to_ps(tokens: List[en.MToken]) -> str:
        return ''.join(t.phonemes + (' ' if t.whitespace else '') for t in tokens).strip()

    @staticmethod
    def waterfall_last(
        tokens: List[en.MToken],
        next_count: int,
        waterfall: List[str] = ['!.?‚Ä¶', ':;', ',‚Äî'],
        bumps: List[str] = [')', '‚Äù']
    ) -> int:
        for w in waterfall:
            z = next((i for i, t in reversed(list(enumerate(tokens))) if t.phonemes in set(w)), None)
            if z is None:
                continue
            z += 1
            if z < len(tokens) and tokens[z].phonemes in bumps:
                z += 1
            if next_count - len(KPipeline.tokens_to_ps(tokens[:z])) <= 510:
                return z
        return len(tokens)

    @staticmethod
    def tokens_to_text(tokens: List[en.MToken]) -> str:
        return ''.join(t.text + t.whitespace for t in tokens).strip()

    def en_tokenize(
        self,
        tokens: List[en.MToken]
    ) -> Generator[Tuple[str, str, List[en.MToken]], None, None]:
        tks = []
        pcount = 0
        for t in tokens:
            # American English: …æ => T
            t.phonemes = '' if t.phonemes is None else t.phonemes#.replace('…æ', 'T')
            next_ps = t.phonemes + (' ' if t.whitespace else '')
            next_pcount = pcount + len(next_ps.rstrip())
            if next_pcount > 510:
                z = KPipeline.waterfall_last(tks, next_pcount)
                text = KPipeline.tokens_to_text(tks[:z])
                logger.debug(f"Chunking text at {z}: '{text[:30]}{'...' if len(text) > 30 else ''}'")
                ps = KPipeline.tokens_to_ps(tks[:z])
                yield text, ps, tks[:z]
                tks = tks[z:]
                pcount = len(KPipeline.tokens_to_ps(tks))
                if not tks:
                    next_ps = next_ps.lstrip()
            tks.append(t)
            pcount += len(next_ps)
        if tks:
            text = KPipeline.tokens_to_text(tks)
            ps = KPipeline.tokens_to_ps(tks)
            yield ''.join(text).strip(), ''.join(ps).strip(), tks

    @staticmethod
    def infer(
        model: KModel,
        ps: str,
        pack: torch.FloatTensor,
        speed: Union[float, Callable[[int], float]] = 1
    ) -> KModel.Output:
        if callable(speed):
            speed = speed(len(ps))
        return model(ps, pack[len(ps)-1], speed, return_output=True)

    def generate_from_tokens(
        self,
        tokens: Union[str, List[en.MToken]],
        voice: str,
        speed: float = 1,
        model: Optional[KModel] = None
    ) -> Generator['KPipeline.Result', None, None]:
        """Generate audio from either raw phonemes or pre-processed tokens.
        
        Args:
            tokens: Either a phoneme string or list of pre-processed MTokens
            voice: The voice to use for synthesis
            speed: Speech speed modifier (default: 1)
            model: Optional KModel instance (uses pipeline's model if not provided)
        
        Yields:
            KPipeline.Result containing the input tokens and generated audio
            
        Raises:
            ValueError: If no voice is provided or token sequence exceeds model limits
        """
        model = model or self.model
        if model and voice is None:
            raise ValueError('Specify a voice: pipeline.generate_from_tokens(..., voice="af_heart")')
        
        pack = self.load_voice(voice).to(model.device) if model else None

        # Handle raw phoneme string
        if isinstance(tokens, str):
            logger.debug("Processing phonemes from raw string")
            if len(tokens) > 510:
                raise ValueError(f'Phoneme string too long: {len(tokens)} > 510')
            output = KPipeline.infer(model, tokens, pack, speed) if model else None
            yield self.Result(graphemes='', phonemes=tokens, output=output)
            return
        
        logger.debug("Processing MTokens")
        # Handle pre-processed tokens
        for gs, ps, tks in self.en_tokenize(tokens):
            if not ps:
                continue
            elif len(ps) > 510:
                logger.warning(f"Unexpected len(ps) == {len(ps)} > 510 and ps == '{ps}'")
                logger.warning("Truncating to 510 characters")
                ps = ps[:510]
            output = KPipeline.infer(model, ps, pack, speed) if model else None
            if output is not None and output.pred_dur is not None:
                KPipeline.join_timestamps(tks, output.pred_dur)
            yield self.Result(graphemes=gs, phonemes=ps, tokens=tks, output=output)

    @staticmethod
    def join_timestamps(tokens: List[en.MToken], pred_dur: torch.LongTensor):
        # Multiply by 600 to go from pred_dur frames to sample_rate 24000
        # Equivalent to dividing pred_dur frames by 40 to get timestamp in seconds
        # We will count nice round half-frames, so the divisor is 80
        MAGIC_DIVISOR = 80
        if not tokens or len(pred_dur) < 3:
            # We expect at least 3: <bos>, token, <eos>
            return
        # We track 2 counts, measured in half-frames: (left, right)
        # This way we can cut space characters in half
        # TODO: Is -3 an appropriate offset?
        left = right = 2 * max(0, pred_dur[0].item() - 3)
        # Updates:
        # left = right + (2 * token_dur) + space_dur
        # right = left + space_dur
        i = 1
        for t in tokens:
            if i >= len(pred_dur)-1:
                break
            if not t.phonemes:
                if t.whitespace:
                    i += 1
                    left = right + pred_dur[i].item()
                    right = left + pred_dur[i].item()
                    i += 1
                continue
            j = i + len(t.phonemes)
            if j >= len(pred_dur):
                break
            t.start_ts = left / MAGIC_DIVISOR
            token_dur = pred_dur[i: j].sum().item()
            space_dur = pred_dur[j].item() if t.whitespace else 0
            left = right + (2 * token_dur) + space_dur
            t.end_ts = left / MAGIC_DIVISOR
            right = left + space_dur
            i = j + (1 if t.whitespace else 0)

    @dataclass
    class Result:
        graphemes: str
        phonemes: str
        tokens: Optional[List[en.MToken]] = None
        output: Optional[KModel.Output] = None
        text_index: Optional[int] = None

        @property
        def audio(self) -> Optional[torch.FloatTensor]:
            return None if self.output is None else self.output.audio

        @property
        def pred_dur(self) -> Optional[torch.LongTensor]:
            return None if self.output is None else self.output.pred_dur

        ### MARK: BEGIN BACKWARD COMPAT ###
        def __iter__(self):
            yield self.graphemes
            yield self.phonemes
            yield self.audio

        def __getitem__(self, index):
            return [self.graphemes, self.phonemes, self.audio][index]

        def __len__(self):
            return 3
        #### MARK: END BACKWARD COMPAT ####

    def __call__(
        self,
        text: Union[str, List[str]],
        voice: Optional[str] = None,
        speed: Union[float, Callable[[int], float]] = 1,
        split_pattern: Optional[str] = r'\n+',
        model: Optional[KModel] = None
    ) -> Generator['KPipeline.Result', None, None]:
        model = model or self.model
        if model and voice is None:
            raise ValueError('Specify a voice: en_us_pipeline(text="Hello world!", voice="af_heart")')
        pack = self.load_voice(voice).to(model.device) if model else None
        
        # Convert input to list of segments
        if isinstance(text, str):
            text = re.split(split_pattern, text.strip()) if split_pattern else [text]
            
        # Process each segment
        for graphemes_index, graphemes in enumerate(text):
            if not graphemes.strip():  # Skip empty segments
                continue
                
            # English processing (unchanged)
            if self.lang_code in 'ab':
                logger.debug(f"Processing English text: {graphemes[:50]}{'...' if len(graphemes) > 50 else ''}")
                _, tokens = self.g2p(graphemes)
                for gs, ps, tks in self.en_tokenize(tokens):
                    if not ps:
                        continue
                    elif len(ps) > 510:
                        logger.warning(f"Unexpected len(ps) == {len(ps)} > 510 and ps == '{ps}'")
                        ps = ps[:510]
                    output = KPipeline.infer(model, ps, pack, speed) if model else None
                    if output is not None and output.pred_dur is not None:
                        KPipeline.join_timestamps(tks, output.pred_dur)
                    yield self.Result(graphemes=gs, phonemes=ps, tokens=tks, output=output, text_index=graphemes_index)
            
            # Non-English processing with chunking
            else:
                # Split long text into smaller chunks (roughly 400 characters each)
                # Using sentence boundaries when possible
                chunk_size = 400
                chunks = []
                
                # Try to split on sentence boundaries first
                sentences = re.split(r'([.!?]+)', graphemes)
                current_chunk = ""
                
                for i in range(0, len(sentences), 2):
                    sentence = sentences[i]
                    # Add the punctuation back if it exists
                    if i + 1 < len(sentences):
                        sentence += sentences[i + 1]
                        
                    if len(current_chunk) + len(sentence) <= chunk_size:
                        current_chunk += sentence
                    else:
                        if current_chunk:
                            chunks.append(current_chunk.strip())
                        current_chunk = sentence
                
                if current_chunk:
                    chunks.append(current_chunk.strip())
                
                # If no chunks were created (no sentence boundaries), fall back to character-based chunking
                if not chunks:
                    chunks = [graphemes[i:i+chunk_size] for i in range(0, len(graphemes), chunk_size)]
                
                # Process each chunk
                for chunk in chunks:
                    if not chunk.strip():
                        continue
                        
                    ps, _ = self.g2p(chunk)
                    if not ps:
                        continue
                    elif len(ps) > 510:
                        logger.warning(f'Truncating len(ps) == {len(ps)} > 510')
                        ps = ps[:510]
                        
                    output = KPipeline.infer(model, ps, pack, speed) if model else None
                    yield self.Result(graphemes=chunk, phonemes=ps, output=output, text_index=graphemes_index)


##### FILE: ./kokoro/kokoro/modules.py #####

# https://github.com/yl4579/StyleTTS2/blob/main/models.py
from .istftnet import AdainResBlk1d
from torch.nn.utils.parametrizations import weight_norm
from transformers import AlbertModel
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F


class LinearNorm(nn.Module):
    def __init__(self, in_dim, out_dim, bias=True, w_init_gain='linear'):
        super(LinearNorm, self).__init__()
        self.linear_layer = nn.Linear(in_dim, out_dim, bias=bias)
        nn.init.xavier_uniform_(self.linear_layer.weight, gain=nn.init.calculate_gain(w_init_gain))

    def forward(self, x):
        return self.linear_layer(x)


class LayerNorm(nn.Module):
    def __init__(self, channels, eps=1e-5):
        super().__init__()
        self.channels = channels
        self.eps = eps
        self.gamma = nn.Parameter(torch.ones(channels))
        self.beta = nn.Parameter(torch.zeros(channels))

    def forward(self, x):
        x = x.transpose(1, -1)
        x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)
        return x.transpose(1, -1)


class TextEncoder(nn.Module):
    def __init__(self, channels, kernel_size, depth, n_symbols, actv=nn.LeakyReLU(0.2)):
        super().__init__()
        self.embedding = nn.Embedding(n_symbols, channels)
        padding = (kernel_size - 1) // 2
        self.cnn = nn.ModuleList()
        for _ in range(depth):
            self.cnn.append(nn.Sequential(
                weight_norm(nn.Conv1d(channels, channels, kernel_size=kernel_size, padding=padding)),
                LayerNorm(channels),
                actv,
                nn.Dropout(0.2),
            ))
        self.lstm = nn.LSTM(channels, channels//2, 1, batch_first=True, bidirectional=True)

    def forward(self, x, input_lengths, m):
        x = self.embedding(x)  # [B, T, emb]
        x = x.transpose(1, 2)  # [B, emb, T]
        m = m.unsqueeze(1)
        x.masked_fill_(m, 0.0)
        for c in self.cnn:
            x = c(x)
            x.masked_fill_(m, 0.0)
        x = x.transpose(1, 2)  # [B, T, chn]
        lengths = input_lengths if input_lengths.device == torch.device('cpu') else input_lengths.to('cpu')
        x = nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)
        self.lstm.flatten_parameters()
        x, _ = self.lstm(x)
        x, _ = nn.utils.rnn.pad_packed_sequence(x, batch_first=True)
        x = x.transpose(-1, -2)
        x_pad = torch.zeros([x.shape[0], x.shape[1], m.shape[-1]], device=x.device)
        x_pad[:, :, :x.shape[-1]] = x
        x = x_pad
        x.masked_fill_(m, 0.0)
        return x


class AdaLayerNorm(nn.Module):
    def __init__(self, style_dim, channels, eps=1e-5):
        super().__init__()
        self.channels = channels
        self.eps = eps
        self.fc = nn.Linear(style_dim, channels*2)

    def forward(self, x, s):
        x = x.transpose(-1, -2)
        x = x.transpose(1, -1)
        h = self.fc(s)
        h = h.view(h.size(0), h.size(1), 1)
        gamma, beta = torch.chunk(h, chunks=2, dim=1)
        gamma, beta = gamma.transpose(1, -1), beta.transpose(1, -1)
        x = F.layer_norm(x, (self.channels,), eps=self.eps)
        x = (1 + gamma) * x + beta
        return x.transpose(1, -1).transpose(-1, -2)


class ProsodyPredictor(nn.Module):
    def __init__(self, style_dim, d_hid, nlayers, max_dur=50, dropout=0.1):
        super().__init__()
        self.text_encoder = DurationEncoder(sty_dim=style_dim, d_model=d_hid,nlayers=nlayers, dropout=dropout)
        self.lstm = nn.LSTM(d_hid + style_dim, d_hid // 2, 1, batch_first=True, bidirectional=True)
        self.duration_proj = LinearNorm(d_hid, max_dur)
        self.shared = nn.LSTM(d_hid + style_dim, d_hid // 2, 1, batch_first=True, bidirectional=True)
        self.F0 = nn.ModuleList()
        self.F0.append(AdainResBlk1d(d_hid, d_hid, style_dim, dropout_p=dropout))
        self.F0.append(AdainResBlk1d(d_hid, d_hid // 2, style_dim, upsample=True, dropout_p=dropout))
        self.F0.append(AdainResBlk1d(d_hid // 2, d_hid // 2, style_dim, dropout_p=dropout))
        self.N = nn.ModuleList()
        self.N.append(AdainResBlk1d(d_hid, d_hid, style_dim, dropout_p=dropout))
        self.N.append(AdainResBlk1d(d_hid, d_hid // 2, style_dim, upsample=True, dropout_p=dropout))
        self.N.append(AdainResBlk1d(d_hid // 2, d_hid // 2, style_dim, dropout_p=dropout))
        self.F0_proj = nn.Conv1d(d_hid // 2, 1, 1, 1, 0)
        self.N_proj = nn.Conv1d(d_hid // 2, 1, 1, 1, 0)

    def forward(self, texts, style, text_lengths, alignment, m):
        d = self.text_encoder(texts, style, text_lengths, m)
        m = m.unsqueeze(1)
        lengths = text_lengths if text_lengths.device == torch.device('cpu') else text_lengths.to('cpu')
        x = nn.utils.rnn.pack_padded_sequence(d, lengths, batch_first=True, enforce_sorted=False)
        self.lstm.flatten_parameters()
        x, _ = self.lstm(x)
        x, _ = nn.utils.rnn.pad_packed_sequence(x, batch_first=True)
        x_pad = torch.zeros([x.shape[0], m.shape[-1], x.shape[-1]], device=x.device)
        x_pad[:, :x.shape[1], :] = x
        x = x_pad
        duration = self.duration_proj(nn.functional.dropout(x, 0.5, training=False))
        en = (d.transpose(-1, -2) @ alignment)
        return duration.squeeze(-1), en

    def F0Ntrain(self, x, s):
        x, _ = self.shared(x.transpose(-1, -2))
        F0 = x.transpose(-1, -2)
        for block in self.F0:
            F0 = block(F0, s)
        F0 = self.F0_proj(F0)
        N = x.transpose(-1, -2)
        for block in self.N:
            N = block(N, s)
        N = self.N_proj(N)
        return F0.squeeze(1), N.squeeze(1)


class DurationEncoder(nn.Module):
    def __init__(self, sty_dim, d_model, nlayers, dropout=0.1):
        super().__init__()
        self.lstms = nn.ModuleList()
        for _ in range(nlayers):
            self.lstms.append(nn.LSTM(d_model + sty_dim, d_model // 2, num_layers=1, batch_first=True, bidirectional=True))
            self.lstms.append(AdaLayerNorm(sty_dim, d_model))
        self.dropout = dropout
        self.d_model = d_model
        self.sty_dim = sty_dim

    def forward(self, x, style, text_lengths, m):
        masks = m
        x = x.permute(2, 0, 1)
        s = style.expand(x.shape[0], x.shape[1], -1)
        x = torch.cat([x, s], axis=-1)
        x.masked_fill_(masks.unsqueeze(-1).transpose(0, 1), 0.0)
        x = x.transpose(0, 1)
        x = x.transpose(-1, -2)
        for block in self.lstms:
            if isinstance(block, AdaLayerNorm):
                x = block(x.transpose(-1, -2), style).transpose(-1, -2)
                x = torch.cat([x, s.permute(1, 2, 0)], axis=1)
                x.masked_fill_(masks.unsqueeze(-1).transpose(-1, -2), 0.0)
            else:
                lengths = text_lengths if text_lengths.device == torch.device('cpu') else text_lengths.to('cpu')
                x = x.transpose(-1, -2)
                x = nn.utils.rnn.pack_padded_sequence(
                    x, lengths, batch_first=True, enforce_sorted=False)
                block.flatten_parameters()
                x, _ = block(x)
                x, _ = nn.utils.rnn.pad_packed_sequence(
                    x, batch_first=True)
                x = F.dropout(x, p=self.dropout, training=False)
                x = x.transpose(-1, -2)
                x_pad = torch.zeros([x.shape[0], x.shape[1], m.shape[-1]], device=x.device)
                x_pad[:, :, :x.shape[-1]] = x
                x = x_pad

        return x.transpose(-1, -2)


# https://github.com/yl4579/StyleTTS2/blob/main/Utils/PLBERT/util.py
class CustomAlbert(AlbertModel):
    def forward(self, *args, **kwargs):
        outputs = super().forward(*args, **kwargs)
        return outputs.last_hidden_state


##### FILE: ./kokoro/kokoro/__main__.py #####

"""Kokoro TTS CLI
Example usage:
python3 -m kokoro --text "The sky above the port was the color of television, tuned to a dead channel." -o file.wav --debug

echo "Bom dia mundo, como v√£o voc√™s" > text.txt
python3 -m kokoro -i text.txt -l p --voice pm_alex > audio.wav

Common issues:
pip not installed: `uv pip install pip`
(Temporary workaround while https://github.com/explosion/spaCy/issues/13747 is not fixed)

espeak not installed: `apt-get install espeak-ng`
"""

import argparse
import wave
from pathlib import Path
from typing import Generator, TYPE_CHECKING

import numpy as np
from loguru import logger

languages = [
    "a",  # American English
    "b",  # British English
    "h",  # Hindi
    "e",  # Spanish
    "f",  # French
    "i",  # Italian
    "p",  # Brazilian Portuguese
    "j",  # Japanese
    "z",  # Mandarin Chinese
]

if TYPE_CHECKING:
    from kokoro import KPipeline


def generate_audio(
    text: str, kokoro_language: str, voice: str, speed=1
) -> Generator["KPipeline.Result", None, None]:
    from kokoro import KPipeline

    if not voice.startswith(kokoro_language):
        logger.warning(f"Voice {voice} is not made for language {kokoro_language}")
    pipeline = KPipeline(lang_code=kokoro_language)
    yield from pipeline(text, voice=voice, speed=speed, split_pattern=r"\n+")


def generate_and_save_audio(
    output_file: Path, text: str, kokoro_language: str, voice: str, speed=1
) -> None:
    with wave.open(str(output_file.resolve()), "wb") as wav_file:
        wav_file.setnchannels(1)  # Mono audio
        wav_file.setsampwidth(2)  # 2 bytes per sample (16-bit audio)
        wav_file.setframerate(24000)  # Sample rate

        for result in generate_audio(
            text, kokoro_language=kokoro_language, voice=voice, speed=speed
        ):
            logger.debug(result.phonemes)
            if result.audio is None:
                continue
            audio_bytes = (result.audio.numpy() * 32767).astype(np.int16).tobytes()
            wav_file.writeframes(audio_bytes)


def main() -> None:
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "-m",
        "--voice",
        default="af_heart",
        help="Voice to use",
    )
    parser.add_argument(
        "-l",
        "--language",
        help="Language to use (defaults to the one corresponding to the voice)",
        choices=languages,
    )
    parser.add_argument(
        "-o",
        "--output-file",
        "--output_file",
        type=Path,
        help="Path to output WAV file",
        required=True,
    )
    parser.add_argument(
        "-i",
        "--input-file",
        "--input_file",
        type=Path,
        help="Path to input text file (default: stdin)",
    )
    parser.add_argument(
        "-t",
        "--text",
        help="Text to use instead of reading from stdin",
    )
    parser.add_argument(
        "-s",
        "--speed",
        type=float,
        default=1.0,
        help="Speech speed",
    )
    parser.add_argument(
        "--debug",
        action="store_true",
        help="Print DEBUG messages to console",
    )
    args = parser.parse_args()
    if args.debug:
        logger.level("DEBUG")
    logger.debug(args)

    lang = args.language or args.voice[0]

    if args.text is not None and args.input_file is not None:
        raise Exception("You cannot specify both 'text' and 'input_file'")
    elif args.text:
        text = args.text
    elif args.input_file:
        file: Path = args.input_file
        text = file.read_text()
    else:
        import sys
        print("Press Ctrl+D to stop reading input and start generating", flush=True)
        text = '\n'.join(sys.stdin)

    logger.debug(f"Input text: {text!r}")

    out_file: Path = args.output_file
    if not out_file.suffix == ".wav":
        logger.warning("The output file name should end with .wav")
    generate_and_save_audio(
        output_file=out_file,
        text=text,
        kokoro_language=lang,
        voice=args.voice,
        speed=args.speed,
    )


if __name__ == "__main__":
    main()
